[{"content":" Mercari Challenge Mercari is Japan’s biggest community-powered shopping website. With the aim of realizing a society where global resources are used carefully and where everyone can live richly, the company has developed a flea market application ‘Mercari’ in Japan and the United States that allows individuals to easily and safely buy and sell goods. Mercari’s challenge is to build an algorithm that automatically suggests the right product prices to sellers on its app.\nPredicting the price of a product is a tough challenge since very similar products having minute differences such as different brand names, additional specifications, quality, demand of the product, etc. can have very different prices. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?\n Figure 1: Image source: https://www.kaggle.com/c/mercari-price-suggestion-challenge/overview  Price prediction gets even more difficult when there is a huge range of products, which is common with most of the online shopping platforms. Mercari’s sellers are allowed to list almost anything on the app. It’s highly challenging to predict the price of almost anything that is listed on online platforms. Lets start to read the data first.\nlibrary(tidyverse) library(data.table) library(quanteda) library(tictoc) library(Matrix) library(xgboost) library(MLmetrics) library(lubridate) library(pracma) data_train \u0026lt;- read_csv(\u0026quot;data/mercari/data-train.csv\u0026quot;) glimpse(data_train) #\u0026gt; Rows: 8,000 #\u0026gt; Columns: 8 #\u0026gt; $ train_id \u0026lt;dbl\u0026gt; 3785, 502, 3429, 3695, 4089, 7885, 3051, 8191, 8661,… #\u0026gt; $ name \u0026lt;chr\u0026gt; \u0026quot;Crystal Flower .925 SP Earrings Bundle\u0026quot;, \u0026quot;NEW IN BO… #\u0026gt; $ item_condition_id \u0026lt;dbl\u0026gt; 2, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1… #\u0026gt; $ category_name \u0026lt;chr\u0026gt; \u0026quot;Women/Jewelry/Earrings\u0026quot;, \u0026quot;Electronics/Cell Phones \u0026amp;… #\u0026gt; $ brand_name \u0026lt;chr\u0026gt; NA, \u0026quot;Belkin\u0026quot;, \u0026quot;MARC JACOBS\u0026quot;, \u0026quot;PINK\u0026quot;, NA, \u0026quot;Nautica\u0026quot;, … #\u0026gt; $ price \u0026lt;dbl\u0026gt; 5, 15, 11, 13, 10, 10, 124, 66, 10, 7, 18, 39, 46, 3… #\u0026gt; $ shipping \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0… #\u0026gt; $ item_description \u0026lt;chr\u0026gt; \u0026quot;Perfect for Spring! Bundle of 2 pair crystal flower… The files consist of product listings. Originally the total size of the data is 1.03 GB. But for demo needs we reduce the number of product to 8000 pieces. Both train and test files have the following data fields:\n name: the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm] item_condition_id: the condition of the items provided by the seller category_name: category of the listing brand_name price: the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn’t exist in test.tsv since that is what you will predict. shipping: 1 if shipping fee is paid by seller and 0 by buyer item_description: the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]  Exploratory Data Analysis head(data_train) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; train_id name item_condition_id category_name brand_name price shipping #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 3785 Crystal… 2 Women/Jewelry/E… \u0026lt;NA\u0026gt; 5 1 #\u0026gt; 2 502 NEW IN … 1 Electronics/Cel… Belkin 15 1 #\u0026gt; 3 3429 Marc Ja… 1 Beauty/Makeup/L… MARC JACO… 11 1 #\u0026gt; 4 3695 Vs pink… 2 Women/Underwear… PINK 13 1 #\u0026gt; 5 4089 Brand n… 1 Women/Other/Oth… \u0026lt;NA\u0026gt; 10 1 #\u0026gt; 6 7885 Mens XX… 2 Men/Tops/T-shir… Nautica 10 0 #\u0026gt; # … with 1 more variable: item_description \u0026lt;chr\u0026gt; For the next step, we will do some Exploratory Data Analysis (EDA) which aims to gain insight and improve our understanding of data by looking at a more detailed perspective, based on our business question. The first one we want to deep dive is price variable. How is it distributed? is it any outliers or anomalies? We can utilize the simple function called summary() to get statistics information.\ndata_train %\u0026gt;% pull(price) %\u0026gt;% summary() #\u0026gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #\u0026gt; 0.00 10.00 17.00 26.59 29.00 1506.00 From the result above, some products cost $0 and the other ones has extreme product prices that are far from the distribution. There seems to be an input error in the data which a price of $0. So we will remove the product with these conditions.\ndata_train \u0026lt;- data_train %\u0026gt;% filter(price != 0)  Data Preparation Next, we will prepare all string data type. If we are dealing or working with string data as a predictor of machine learning model, and we know the ‘R case sensitive’ characteristic, we need to convert all character to lower case format. So the word Algoritma and algoritma has the same meaning for our program. We can simply use mutate_if(is.character, tolower) syntax on our data.\nThe item description column contains blank and no decription yet which is the same meaning. So lets convert it to a single word null.\ndata_train \u0026lt;- data_train %\u0026gt;% mutate_if(is.character, tolower) %\u0026gt;% mutate( item_description = ifelse( item_description == \u0026quot;\u0026quot; | item_description == \u0026quot;no description yet\u0026quot;, \u0026quot;null\u0026quot;, item_description ) ) head(data_train, 6) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; train_id name item_condition_id category_name brand_name price shipping #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 3785 crystal… 2 women/jewelry/e… \u0026lt;NA\u0026gt; 5 1 #\u0026gt; 2 502 new in … 1 electronics/cel… belkin 15 1 #\u0026gt; 3 3429 marc ja… 1 beauty/makeup/l… marc jaco… 11 1 #\u0026gt; 4 3695 vs pink… 2 women/underwear… pink 13 1 #\u0026gt; 5 4089 brand n… 1 women/other/oth… \u0026lt;NA\u0026gt; 10 1 #\u0026gt; 6 7885 mens xx… 2 men/tops/t-shir… nautica 10 0 #\u0026gt; # … with 1 more variable: item_description \u0026lt;chr\u0026gt; Separate Category Name Observe that the entries of category_name are separated into subcategories by the ‘/’ symbol. How many subcategories are there? we can run following code:\ntemp_text \u0026lt;- \u0026quot;men/tops/t-shirts\u0026quot; str_count(temp_text, pattern = \u0026quot;/\u0026quot;) #\u0026gt; [1] 2 We use str_count function from stringr package to count the number of matches in a string. With the sample text above, we get the result two, which mean the sample consist of three subcategories (‘result + 1’). Let’s apply to our category_name column.\ndata_train %\u0026gt;% pull(category_name) %\u0026gt;% str_count(pattern = \u0026quot;/\u0026quot;) %\u0026gt;% unique() #\u0026gt; [1] 2 4 NA 3 The output shows the maximum number of subcategories is five. Also there are entries which no have values which need to convert as ‘unknown’ in the next step.\nWe want record each subcategories name as a single column. Then, we prepare new columns names and use separate() function to separate a character column into multiple columns with a ‘/’ separator. zTo make it clear how the function works, we will use sample data and only focus on ‘category_name’.\nsubcat \u0026lt;- c(\u0026#39;cat_1\u0026#39;, \u0026#39;cat_2\u0026#39;, \u0026#39;cat_3\u0026#39;, \u0026#39;cat_4\u0026#39;, \u0026#39;cat_5\u0026#39;) temp_category \u0026lt;- data_train %\u0026gt;% select(name, category_name) %\u0026gt;% separate(col = category_name, into = subcat, sep = \u0026quot;/\u0026quot;, remove = FALSE) tail(temp_category) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; name category_name cat_1 cat_2 cat_3 cat_4 cat_5 #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; #\u0026gt; 1 body control beauty/skin care/body beauty skin care body \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 2 tide pods bu… home/cleaning supplies/h… home cleaning… househol… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 3 xs vs pink w… women/athletic apparel/t… women athletic… tracksui… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 4 apple iphone… electronics/cell phones … elect… cell pho… cell pho… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 5 butterfly wi… home/home décor/home déc… home home déc… home déc… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; #\u0026gt; 6 crystal clea… electronics/cell phones … elect… cell pho… cases, c… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Then, lest apply to the original data.\ndata_train \u0026lt;- data_train %\u0026gt;% separate(col = category_name, into = subcat, sep = \u0026quot;/\u0026quot;) Next, we will replace empty entries in data with ‘unknown’ character. Supposedly, if we check again with anyNA() function, there are no more NA data. Also, for the sake of decreasing computation cost, we will convert some of categorical columns to factor type.\ndata_train \u0026lt;- data_train %\u0026gt;% replace(is.na(.), \u0026quot;unknown\u0026quot;) %\u0026gt;% mutate(brand_name = ifelse(brand_name == \u0026quot;\u0026quot;, \u0026quot;unknwon\u0026quot;, brand_name)) %\u0026gt;% mutate_at(.vars = c(\u0026quot;item_condition_id\u0026quot;, \u0026quot;brand_name\u0026quot;, \u0026quot;shipping\u0026quot;, subcat), as.factor) anyNA(data_train) #\u0026gt; [1] FALSE We can compile all data preparation above as a reproducible function. If any new data input, we no need to execute line by line with the same command, but simply apply the function we created. We’ll apply this function to test dataset later.\ndata_prep \u0026lt;- function(data){ data_clean \u0026lt;- data %\u0026gt;% filter(price != 0) %\u0026gt;% mutate_if(is.character, tolower) %\u0026gt;% mutate( item_description = ifelse( item_description == \u0026quot;\u0026quot; | item_description == \u0026quot;no description yet\u0026quot;, \u0026quot;null\u0026quot;, item_description ) ) %\u0026gt;% separate(col = category_name, into = subcat, sep = \u0026quot;/\u0026quot;) %\u0026gt;% replace(is.na(.), \u0026quot;unknown\u0026quot;) %\u0026gt;% mutate(brand_name = ifelse(brand_name == \u0026quot;\u0026quot;, \u0026quot;unknwon\u0026quot;, brand_name)) %\u0026gt;% mutate_at(.vars = c(\u0026quot;item_condition_id\u0026quot;, \u0026quot;brand_name\u0026quot;, \u0026quot;shipping\u0026quot;, subcat), as.factor) return(data_clean) }  Document Feature Matrix Our task is to predict product price from all information entered by the merchant included item description, name, and category. To do so, we mine features from those textual data and fit to a machine learning model. Following are some approach to make our program understand every single words that entries to the system:\nDocument Feature Matrix One Hot Encoding Data Sparse Matrix  Document Feature Matrix or familiar called as Document Term Matrix is an important representation for text analysis. Each row of the matrix is a document vector which is our each product, and the column represent every term in the entire dictionary.\nSome documents may not contain certain terms, so these matrix are sparse. The value in each cell of the matrix is the frequency term. This value is often a weighted term frequency, typically using Term Frequency-Inverse Document Frequency (TF-IDF)\nWhy TFIDF?\nTerm Frequency approach to determine the weight of each term in a document based on the number of occurrences in the document. The greater the number of occurrences (high TF), the greater its weight in the document. But, there are not important words that appear several time in the document which can be biased during modelling.\nSo Inverse Document Frequency approach come up to solve that problem. Inverse Document Frequency (IDF) to reduce the dominance of words that often appear in various documents. This step is necessary because words that appear a lot in various document can be considered as general terms so the value will set to ‘not important’. TF-IDF to measure how important a word is in the corpus.\nbuild_dfm \u0026lt;- function(x, n = 1) { mat \u0026lt;- dfm( x, tolower = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove = stopwords(\u0026quot;english\u0026quot;), ngrams = n ) mat \u0026lt;- dfm_tfidf(mat) return(mat) } After we do Document Feature Matrix for column name and item_description, we combine them into one data matrix. So imagine if the nike column contains the word ‘nike’ and the item_description column also contains the word ‘nike’, the result may be misleading. For instance, please look the sample below:\n Figure 2: Original sample data   Figure 3: DFM result  It’s too difficult for our machine to understand the word ‘nike’ which is the product name, and the word ‘nike’ as a item description. So the solution is to paste the context of each column to the rest of the features, to define a new colnames more unique.\ntic() dfm_item_description \u0026lt;- build_dfm(x = data_train$item_description) dfm_item_description@Dimnames[[2]] \u0026lt;- paste0(\u0026quot;desc_\u0026quot;, dfm_item_description@Dimnames[[2]]) toc() #\u0026gt; elapsed time is 0.846000 seconds tic() dfm_name \u0026lt;- build_dfm(x = data_train$name) dfm_name@Dimnames[[2]] \u0026lt;- paste0(\u0026quot;name_\u0026quot;, dfm_name@Dimnames[[2]]) toc() #\u0026gt; elapsed time is 0.381000 seconds  One-hot Encoding Categorical data refers to variables that are made up of label values, for example, a “performance level” variable could have the values “low“, “medium, and “high”. One-hot encoding is a scheme of vectorization where each category in a categorical variable is converted into a vector of length equal to the number of data points. The vector contains a value of 1 against each data point that belongs to the category corresponding to the vector and contains 0 otherwise. To make it clearer, look at the following example.\n Figure 4: One-hot encoding concept  We will converted all the categorical variables (item_condition, shipping, brand_name, cat_1, …, cat_5) to their one-hot encoded vectors. Example code is shown below:\ntemp \u0026lt;- sparse.model.matrix(~ brand_name + cat_1 + cat_2 + cat_3 + cat_4 + cat_5, data = data_train[1:10, c(\u0026quot;item_condition_id\u0026quot;, \u0026quot;brand_name\u0026quot;, \u0026quot;shipping\u0026quot;, subcat)]) temp %\u0026gt;% as.matrix() %\u0026gt;% as.data.frame() %\u0026gt;% select(1:6) %\u0026gt;% head() #\u0026gt; (Intercept) brand_name90 degree by reflex brand_namea bathing ape #\u0026gt; 1 1 0 0 #\u0026gt; 2 1 0 0 #\u0026gt; 3 1 0 0 #\u0026gt; 4 1 0 0 #\u0026gt; 5 1 0 0 #\u0026gt; 6 1 0 0 #\u0026gt; brand_namea plus child supply brand_namea+d brand_nameabercrombie \u0026amp; fitch #\u0026gt; 1 0 0 0 #\u0026gt; 2 0 0 0 #\u0026gt; 3 0 0 0 #\u0026gt; 4 0 0 0 #\u0026gt; 5 0 0 0 #\u0026gt; 6 0 0 0 Since we know our data will probably have a lot of zero values, so we will use the sparse matrix to store the data we have processed. A sparse matrix is a matrix that is comprised of mostly zero values.\n A matrix is sparse if many of its coefficients are zero. The interest in sparsity arises because its exploitation can lead to enormous computational savings and because many large matrix problems that occur in practice are sparse.\n Often you may deal with large matrices that are sparse with a few non-zero elements. In such scenarios, keeping the data in full dense matrix and working with it is not efficient.\nA better way to deal with such sparse matrices is to use the special data structures that allows to store the sparse data efficiently. In R, the matrix package offers great solutions to deal with large sparse matrices.\nLet’s see comparison between dense matrix and sparse matrix in term of the size. Let us create a dummy data and randomly select the indices and make them to contain zeroes.\ndata \u0026lt;- rnorm(1e6) zero_index \u0026lt;- sample(1e6)[1:9e5] data[zero_index] \u0026lt;- 0 Now we have created a vector of million elements, but 90% of the elements are zeros. Let us make it into a dense matrix.\nmat \u0026lt;- matrix(data, ncol=1000) mat[1:5,1:5] #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 0.000000 0.0000000 0.000000 0 0.2325384 #\u0026gt; [2,] -1.102067 0.6529056 0.000000 0 0.0000000 #\u0026gt; [3,] 0.000000 0.0000000 0.000000 0 0.0000000 #\u0026gt; [4,] 0.000000 0.0000000 0.000000 0 -0.4474099 #\u0026gt; [5,] 0.000000 0.0000000 0.792445 0 0.0000000 We can use R function object.size and check the size of the dense matrix.\nprint(object.size(mat),units=\u0026quot;auto\u0026quot;) #\u0026gt; 7.6 Mb Let us use sparse matrix library to convert the dense matrix to sparse matrix. We can see that elements with no values are shown as dots.\nmat_sparse \u0026lt;- Matrix(mat, sparse = TRUE) mat_sparse[1:5, 1:5] #\u0026gt; 5 x 5 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot; #\u0026gt; #\u0026gt; [1,] . . . . 0.2325384 #\u0026gt; [2,] -1.102067 0.6529056 . . . #\u0026gt; [3,] . . . . . #\u0026gt; [4,] . . . . -0.4474099 #\u0026gt; [5,] . . 0.792445 . . It tells us that our sparse matrix belongs to a class “dgCMatrix”. The sparse matrix type “dgCMatrix” refers to double sparse matrix stored in CSC, Compressed Sparse Column format. A sparse matrix in CSC format is column-oriented format and it is implemented such that the non-zero elements in the columns are sorted into increasing row order. Let us check the size of our sparse matrix.\nprint(object.size(mat_sparse),units=\u0026quot;auto\u0026quot;) #\u0026gt; 1.1 Mb The sparse matrix stores the same data in just about 1 Mb, way more memory efficient than the dense matrix. About seven times smaller than the dense matrix. So lets apply in our data train:\ntic() one_hot_train \u0026lt;- sparse.model.matrix( ~ item_condition_id + shipping + brand_name + cat_1 + cat_2 + cat_3 + cat_4 + cat_5, data = data_train[c(\u0026quot;item_condition_id\u0026quot;, \u0026quot;brand_name\u0026quot;, \u0026quot;shipping\u0026quot;, subcat)]) toc() #\u0026gt; elapsed time is 0.064000 seconds Next, we will change object type of dfm_item_description and dfm_name as a dgCMatrix then combine them as one data that ready for modelling.\nclass(dfm_item_description) \u0026lt;- class(one_hot_train) class(dfm_name) \u0026lt;- class(one_hot_train) tic() data_train_sparse \u0026lt;- cbind( one_hot_train, dfm_item_description, dfm_name) rownames(data_train_sparse) \u0026lt;- NULL toc() #\u0026gt; elapsed time is 0.012000 seconds   Modelling with XGBoost XGBoost was formulated by Tianqi Chen which started as a research project a part of The Distributed Deep Machine Leaning Community (DMLC) group. XGBoost is one of popular algorithm because it has been the winning algorithm in a number of recent Kaggle competitions. XGBoost is a specific implementation of the Gradient Boosting Model which uses more accurate approximations to find the best tree model. XGBoost specifically used a more regularized model formalization to control overfitting, which gives it better perfomance.\nConcept Xgboost works through the system optimization:\n1. Parallelized tree building\nXGBoost approaches the process of sequential tree building using parallelized implementation.\n2. Tree pruning\nUnlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree up to max_depth and then prune backward until the improvement in loss function is below a threshold.\n3. Cache awareness and out of core computing\nXGBoost has been designed to efficiently reduce computing time and allocate an optimal usage of memory resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‘out-of-core’ computing optimize available disk space while handling big data-frames that do not fit into memory.\n4. Regularization\nThe biggest advantage of XGBoost is regularization. Regularization is a technique used to avoid overfitting in linear and tree based models which limits, regulates or shrink the estimated coefficient towards zero.\n5. Handles missing value\nThis algorithm has important features of handling missing values by learns the best direction for missing values. The missing values are treated them to combine a sparsity-aware split finding algorithm to handle different types of sparsity patterns in data.\n6. Built-in cross validation\nThe algorithm comes with built in cross validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\n Parameter There is no benchmark to define the ideal parameters because it will depend on your data and specific problem. XGBoost parameters can defined into three categories:\nFor more detail parameter, the full list of possible parameters is available on the documentation XGBoost Parameters\nGeneral Controls the booster type in the model which eventually drives overall functioning.\nbooster  For regression problems, we can use gbtree and gblinear. In gblinear, it builds a generalized linear model and optimizes it using regularization and gradient descent. The next model will built on residuals generated by previous iterations.\nnthread  To enable parallel computing. The default is the maximum number of threads available.\nverbosity (logging)  Verbosity to display warning messages. The default value is 1 (warning), 0 for silent, 2 for info, and 3 for debug.\n Boosting Parameter Controls the performance of the selected booster\neta (alias learning_rate)  The range of eta is 0 to 1 and default value is 0.3. It controls the maximum number of iterations, the lower eta will generate the slower computation.\ngamma (alias min_split_loss)  The range of gamma is 0 to infinite and default value is 0 (no regularization). The higher gamma is the higher regularization, regularization means penalizing large coefficients that don’t improve the model’s performance.\nmax_depth  Maximum depth of a tree. The range of max_depth is 0 to infinite and default value is 6, increasing this value will make the model more complex and more likely to overfit.\nmin_child_weight  The range of min_child_weight is 0 to infinite and default value is 1. If the leaf node has a minimum sum of instance weight lower than min_child_weight in the tree partition step than the process of splitting the tree will stop growing.\nsubsample  The range of subsample is 0 to 1 and default value is 1. It controls the number of ratio observations to a tree. If the value is set to 0.5 means that XGboost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. subsample will occur once in every boosting iteration.\ncolsample_bytree  The range of colsample_bytree is 0 to 1 and default value is 1. It controls the subsample ratio of columns when constructing each tree.\n  Learning Task Parameter Sets and evaluates the learning process of booster from the given data.\nobjective   reg:squarederror for regression with squared loss binary:logistic for binary classification, output probability  eval_metric  Evaluation metrics for validation data, a default metric will be assigned according to objective:\n rmse for regression logloss for classification   Modelling data_train_xgb \u0026lt;- xgb.DMatrix(data = data_train_sparse, label = data_train$price) Let’s build a model and implement a few parameters that can affect our model’s performance and training speed.\ntic() model \u0026lt;- xgboost(data = data_train_xgb, nround = 500, objective = \u0026quot;reg:squarederror\u0026quot;, verbose = FALSE) toc() #\u0026gt; elapsed time is 11.889000 seconds model$evaluation_log %\u0026gt;% ggplot(aes(x = iter, y = train_rmse)) + geom_line() + labs(title = \u0026quot;Model evaluation log\u0026quot;, y = \u0026quot;RMSE data train\u0026quot;, x = \u0026quot;Iteration\u0026quot;)  From the graph above, can we say that the curve is not yet fully convergent? if yes, then that’s a good sign our model can still be improved by increasing the number of iterations. for those of you who are curious, you can do it by yourself, because it is quite time consuming.\n Model Evaluation pred_train \u0026lt;- predict(model, data_train_sparse) result_train \u0026lt;- data_train %\u0026gt;% select(actual = price) %\u0026gt;% mutate(prediction = pred_train) result_train %\u0026gt;% sample_n(10) #\u0026gt; # A tibble: 10 x 2 #\u0026gt; actual prediction #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 15 12.8 #\u0026gt; 2 20 18.8 #\u0026gt; 3 24 22.7 #\u0026gt; 4 24 21.0 #\u0026gt; 5 34 30.9 #\u0026gt; 6 45 37.9 #\u0026gt; 7 14 20.4 #\u0026gt; 8 40 31.5 #\u0026gt; 9 10 17.5 #\u0026gt; 10 26 21.3  Mean Absolute Error  There are many ways of measuring a model’s accuracy. However, the Mean Absolute Error, also known as MAE, is one of the many metrics for summarizing and assessing the quality of a machine learning model, especially for regression task. In MAE the error is calculated as an average of absolute differences between the target values and the predictions.\n\\[MAE = \\frac{1}{n}\\sum_{t=1}^{n}|e_t|\\]\n Root Mean Squared Error  RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.\n\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}e_t^2}\\]\n Mean Absolute Percentage Error  MAPE measures the accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values.\n\\[MAPE = \\frac{100\\%}{n}\\sum_{t=1}^{n}\\left |\\frac{e_t}{y_t}\\right|\\]\nrmse_train \u0026lt;- RMSE(y_pred = result_train$prediction, y_true = result_train$actual) rmse_train #\u0026gt; [1] 7.225152 result_train %\u0026gt;% sample_n(150) %\u0026gt;% mutate(no = 1:150) %\u0026gt;% pivot_longer(cols = c(actual, prediction), names_to = \u0026quot;label\u0026quot;) %\u0026gt;% ggplot(aes(y = value)) + geom_line(aes(x = no, col = label)) + scale_color_manual(values = c(\u0026quot;firebrick\u0026quot;, \u0026quot;dodgerblue\u0026quot;)) + labs(x = \u0026quot;Row indices\u0026quot;, y = \u0026quot;Product price\u0026quot;, title = \u0026quot;Comparasion actual vs prediction price\u0026quot;, subtitle = \u0026quot;Sample of data train\u0026quot;, caption = paste(\u0026quot;RMSE:\u0026quot;, round(rmse_train, 2))) + theme(legend.position = \u0026quot;bottom\u0026quot;)  Predict on Test Data After develop model machine learning on train data set, what is the next step?\n Pick a final model based on an evaluation criteria (the best accurate model)\n Obtain an unbiased measurement of the model’s accuracy by predicting on test set data  The idea of obtaining an unbiased estimate of our model’s out-of-sample performance is an important one as it is often the case that the in-sample error (the error you obtain from running your algorithm on the dataset it was trained on) is optimistic and tuned / adapted in a particular way to minimize the error in the training sample.\nTherefore - the in-sample error is not a good representation or indication of how our model will perform when it is applied on unseen data.\nAnother way to think about is that our training data has two components to it: signal and noise. The goal of machine learning is to identify the signal but be robust enough to avoid modeling the noise component of the data.\nWhen we build a model, we want to know that our model is not overly adapted to the data set to the point that it captures both the signal and noise, a phenomenon known as “overfitting”. When our model is guilty of overfitting, the in-sample accuracy will be very high (in some cases ~100%) but fail to perform on unseen data. The idea is to strike the right balance between accuracy (don’t underfit) and robustness to noise (don’t overfit).\nData Preparation Let’s us import and do some data pre-processing like we did before in training dataset.\ndata_test \u0026lt;- read_csv(\u0026quot;data/mercari/data-test.csv\u0026quot;) glimpse(data_test) #\u0026gt; Rows: 1,595 #\u0026gt; Columns: 8 #\u0026gt; $ train_id \u0026lt;dbl\u0026gt; 3429, 8191, 9731, 822, 2346, 1330, 8172, 5997, 6514,… #\u0026gt; $ name \u0026lt;chr\u0026gt; \u0026quot;Marc Jacobs lipgloss Sugar Sugar\u0026quot;, \u0026quot;Southern Shirt … #\u0026gt; $ item_condition_id \u0026lt;dbl\u0026gt; 1, 3, 1, 1, 1, 3, 2, 3, 3, 2, 1, 1, 1, 2, 2, 1, 3, 3… #\u0026gt; $ category_name \u0026lt;chr\u0026gt; \u0026quot;Beauty/Makeup/Lips\u0026quot;, \u0026quot;Women/Coats \u0026amp; Jackets/Fleece … #\u0026gt; $ brand_name \u0026lt;chr\u0026gt; \u0026quot;MARC JACOBS\u0026quot;, NA, NA, \u0026quot;Louis Vuitton\u0026quot;, \u0026quot;Mega Bloks\u0026quot;… #\u0026gt; $ price \u0026lt;dbl\u0026gt; 11, 66, 18, 39, 24, 305, 12, 5, 14, 350, 10, 8, 39, … #\u0026gt; $ shipping \u0026lt;dbl\u0026gt; 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1… #\u0026gt; $ item_description \u0026lt;chr\u0026gt; \u0026quot;Free shipping Brand new / travel size (no box) Shad… Recall, previously we made a custom function to prepare our unseen data. So, its time we use it on test data set. its look very straightforward, we just simply call the function and set the desired data.\ndata_test \u0026lt;- data_prep(data = data_test) head(data_test) #\u0026gt; # A tibble: 6 x 12 #\u0026gt; train_id name item_condition_… cat_1 cat_2 cat_3 cat_4 cat_5 brand_name price #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 3429 marc… 1 beau… make… lips unkn… unkn… marc jaco… 11 #\u0026gt; 2 8191 sout… 3 women coat… flee… unkn… unkn… unknown 66 #\u0026gt; 3 9731 clip… 1 women wome… hair… unkn… unkn… unknown 18 #\u0026gt; 4 822 loui… 1 women wome… wall… unkn… unkn… louis vui… 39 #\u0026gt; 5 2346 turt… 1 kids toys buil… unkn… unkn… mega bloks 24 #\u0026gt; 6 1330 ipho… 3 elec… cell… cell… unkn… unkn… apple 305 #\u0026gt; # … with 2 more variables: shipping \u0026lt;fct\u0026gt;, item_description \u0026lt;chr\u0026gt;  Document Feature Matrix Here are the same things like we did on training set data. Build a document feature matrix on unseen data. However, there are some adjustments later regarding to the dictionary/corpus. Right now, take your time to remembering the meaning of each command we used.\ntic() dfm_item_description_test \u0026lt;- build_dfm(x = data_test$item_description) dfm_item_description_test@Dimnames[[2]] \u0026lt;- paste0(\u0026quot;desc_\u0026quot;, dfm_item_description_test@Dimnames[[2]]) toc() #\u0026gt; elapsed time is 0.313000 seconds tic() dfm_name_test \u0026lt;- build_dfm(x = data_test$name) dfm_name_test@Dimnames[[2]] \u0026lt;- paste0(\u0026quot;name_\u0026quot;, dfm_name_test@Dimnames[[2]]) toc() #\u0026gt; elapsed time is 0.153000 seconds tic() data_test_sparse \u0026lt;- sparse.model.matrix( ~ item_condition_id + shipping + cat_1 + cat_2 + cat_3 + cat_4 + cat_5, data = data_test[c(\u0026quot;item_condition_id\u0026quot;, \u0026quot;brand_name\u0026quot;, \u0026quot;shipping\u0026quot;, subcat)] ) toc() #\u0026gt; elapsed time is 0.048000 seconds class(dfm_item_description_test) \u0026lt;- class(data_test_sparse) class(dfm_name_test) \u0026lt;- class(data_test_sparse) Combine data one-hot encoding, sparse matrix item description, and sparse matrix name product of our test data set.\ntic() data_test_sparse \u0026lt;- cbind( data_test_sparse, dfm_item_description_test, dfm_name_test) toc() #\u0026gt; elapsed time is 0.020000 seconds  Features Matching Classic problem when dealing with text data predictors is, thare are words that do not appear in new data (test data set). So the dimension between training set and testing set data is different. if we force the model to predict the data, obviously it will error.\ndata_train_sparse@Dim #\u0026gt; [1] 7995 22754 data_test_sparse@Dim #\u0026gt; [1] 1593 9034 Look, total training data columns is 22756 while the test data only 9035. Of course, we need to equate the features of training set and testing set data. First we need to check, which training data columns is not in the test data? following are the commands we used.\nselect_empty \u0026lt;- data_train_sparse@Dimnames[[2]][!(data_train_sparse@Dimnames[[2]] %in% data_test_sparse@Dimnames[[2]])] Then, we generate a new sparse matrix from the result above. Replace all missing value with 0, because in fact the test data does not contain some of these features.\ntic() data_test_empty \u0026lt;- setNames(data.frame(matrix(ncol = length(select_empty), nrow = nrow(data_test))), select_empty) %\u0026gt;% replace(is.na(.), 0) %\u0026gt;% as.matrix() %\u0026gt;% as(\u0026quot;sparseMatrix\u0026quot;) toc() #\u0026gt; elapsed time is 13.025000 seconds Lets combine the data and check the dimension again.\ndata_test_sparse \u0026lt;- cbind( data_test_sparse, data_test_empty ) data_test_sparse@Dim #\u0026gt; [1] 1593 22754 We can see that the column size is same with training data. Not finished yet, we need to match the order of test data columns as in the training set data. Why? this due to the functional requirements of the XGBoost model object it need the same order of columns as learned. Of course this will take quite a while to process.\ndf_test_complete \u0026lt;- data_test_sparse %\u0026gt;% as.matrix() %\u0026gt;% as.data.frame() data_test_sparse \u0026lt;- df_test_complete[,data_train_sparse@Dimnames[[2]]] %\u0026gt;% as.matrix() %\u0026gt;% as(\u0026quot;sparseMatrix\u0026quot;) rownames(data_test_sparse) \u0026lt;- NULL Do not forget to convert as xgb DMatrix.\ndata_test_xgb \u0026lt;- xgb.DMatrix(data_test_sparse)  Model Evaluation Data Test We have finished preparing new data. Now, we can move to prediction step and evaluation model. Let’s use the generic predict() function to predict with the model we’ve constructed, on the test set data to get a sense of it’s performance on unseen data:\npred_test \u0026lt;- predict(model, data_test_xgb) result_test \u0026lt;- data_test %\u0026gt;% select(actual = price) %\u0026gt;% mutate(prediction = pred_test) result_test #\u0026gt; # A tibble: 1,593 x 2 #\u0026gt; actual prediction #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 11 14.2 #\u0026gt; 2 66 60.1 #\u0026gt; 3 18 24.3 #\u0026gt; 4 39 48.1 #\u0026gt; 5 24 25.5 #\u0026gt; 6 305 318. #\u0026gt; 7 12 18.6 #\u0026gt; 8 5 13.4 #\u0026gt; 9 14 18.0 #\u0026gt; 10 350 254. #\u0026gt; # … with 1,583 more rows rmse_test \u0026lt;- RMSE(y_pred = result_test$prediction, y_true = result_test$actual) rmse_test #\u0026gt; [1] 13.79102 result_test %\u0026gt;% sample_n(150) %\u0026gt;% mutate(no = 1:150) %\u0026gt;% pivot_longer(cols = c(actual, prediction), names_to = \u0026quot;label\u0026quot;) %\u0026gt;% ggplot(aes(y = value)) + geom_line(aes(x = no, col = label)) + scale_color_manual(values = c(\u0026quot;firebrick\u0026quot;, \u0026quot;dodgerblue\u0026quot;)) + labs( x = \u0026quot;Row indices\u0026quot;, y = \u0026quot;Product price\u0026quot;, title = \u0026quot;Comparasion actual vs prediction price\u0026quot;, subtitle = \u0026quot;Sample of data test\u0026quot;, caption = paste(\u0026quot;RMSE:\u0026quot;, round(rmse_test, 2)) ) + theme(legend.position = \u0026quot;bottom\u0026quot;) A 13.79 RMSE on unseen data! it seems the model is good enough to predict prices on the new data.\n    ","permalink":"http://ahmadhusain.in/post/2021-06-22-mercari-price-suggestion/","summary":"Mercari Challenge Mercari is Japan’s biggest community-powered shopping website. With the aim of realizing a society where global resources are used carefully and where everyone can live richly, the company has developed a flea market application ‘Mercari’ in Japan and the United States that allows individuals to easily and safely buy and sell goods. Mercari’s challenge is to build an algorithm that automatically suggests the right product prices to sellers on its app.","title":"Mercari Price Suggestion"},{"content":"\r\rBackground\rIn the business process, the marketing team has a role to play in increasing the brand awareness of a product. When the marketing team’s efforts are successful to get the customer’s attention to find out the product, it will generate a lead. In a simple way, leads are people who are interested in a business product.\nIn the digital era, leads can be interpreted as people who visit websites directly or through advertisements, people who like, share content or product campaigns. Then, these prospective customers will be saved and then directed to the sales team. Lots of efforts can be made to generate a lead. Starting from creating creative content, advertising, writing articles, distributing e-books, promo codes and so on.\n\rModeling\rlibrary(tidyverse)\rlibrary(CausalImpact)\rlibrary(readxl)\rlibrary(forecast)\rlibrary(TSstudio)\rlibrary(lubridate)\rThe data comes from the results of a website googleanalytics. For post-period modeling, the previous 37 days will be used as training data. The business question is that on the next day, after the campaign conducted, is it getting a significant increase in leads?\ndata \u0026lt;- read_csv(\u0026quot;http://bit.ly/causal-impact-data\u0026quot;)\rglimpse(data)\r#\u0026gt; Rows: 54\r#\u0026gt; Columns: 9\r#\u0026gt; $ page \u0026lt;chr\u0026gt; \u0026quot;9fb22be32c347a5acd1d3724b0dae726\u0026quot;, \u0026quot;9fb22be32c34...\r#\u0026gt; $ datetime \u0026lt;dbl\u0026gt; 20180516, 20180517, 20180518, 20180519, 20180520,...\r#\u0026gt; $ page_display \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r#\u0026gt; $ unique_views \u0026lt;dbl\u0026gt; 1456, 1392, 1281, 658, 616, 1691, 1488, 1316, 122...\r#\u0026gt; $ average_page_time \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r#\u0026gt; $ tickets \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r#\u0026gt; $ bounce_rate \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r#\u0026gt; $ exit_percentage \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\r#\u0026gt; $ page_value \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\rWe select several columns that are the focus of this analysis, datetime (daily) and unique views which contain information on how many people visited the website page.\nactual \u0026lt;- data %\u0026gt;% mutate(\rdatetime = lubridate::as_datetime(as.character(datetime))\r) %\u0026gt;% dplyr::select(datetime, unique_views) %\u0026gt;% na.omit()\rhead(actual)\r#\u0026gt; # A tibble: 6 x 2\r#\u0026gt; datetime unique_views\r#\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 2018-05-16 00:00:00 1456\r#\u0026gt; 2 2018-05-17 00:00:00 1392\r#\u0026gt; 3 2018-05-18 00:00:00 1281\r#\u0026gt; 4 2018-05-19 00:00:00 658\r#\u0026gt; 5 2018-05-20 00:00:00 616\r#\u0026gt; 6 2018-05-21 00:00:00 1691\r\rNext we will do some exploratory analysis, which day is the most leads generation?\n\ractual %\u0026gt;% mutate(\rwdays = lubridate::wday(datetime, label = TRUE)\r) %\u0026gt;% group_by(wdays) %\u0026gt;% summarise(total_views = sum(unique_views)) %\u0026gt;% ungroup() %\u0026gt;% mutate(\rlabel = scales::comma(total_views)\r) %\u0026gt;% ggplot(\rmapping = aes(x = wdays, y = total_views)\r) +\rgeom_col(fill = \u0026quot;steelblue\u0026quot;, alpha = 0.7) +\rlabs(\rtitle = \u0026quot;Total Views Per Days\u0026quot;,\rsubtitle = \u0026quot;Period: May to July\u0026quot;,\ry = NULL,\rx = \u0026quot;Day of Week\u0026quot;\r) +\rgeom_text(\raes(label = label, y = total_views + max(total_views) * 0.075) , size = 3\r) +\rtheme_minimal()\rLeads generation is highest on Wednesday. This information can be taken into consideration in determining the day to start the campaign.\nNext we will be subset 37 days before the campaign and saved to the pre_campaign object. So we can see the movement of leads and think about the increase that has been generated since the campaign was initiated.\npre_campaign \u0026lt;- actual %\u0026gt;% slice(1:37)\rNext, create time series objects and do modeling to make a banchmark of leads that we can get if we don’t use campaigns. We will use Holtwinter as a method to forecast the next 16 days.\nts_campaign \u0026lt;- ts(pre_campaign$unique_views, frequency = 7)\rfit_hw \u0026lt;- HoltWinters(ts_campaign)\rforecast \u0026lt;- forecast(fit_hw, 16)\rWe append the data for the period before the campaign and the results of the forecast which are saved to the append_data object.\nforecast_data \u0026lt;- data_frame(\rdatetime = as_datetime(\rseq.Date(\rfrom = as.Date(\u0026quot;2018-06-24\u0026quot;),\rby = \u0026quot;day\u0026quot;,\rlength.out = 16)\r),\runique_views = forecast$mean\r)\rappend_data \u0026lt;- pre_campaign %\u0026gt;% bind_rows(forecast_data)\rggplot(data = append_data, mapping = aes(x = datetime, y = unique_views)) +\rgeom_line(col = \u0026quot;steelblue\u0026quot;, alpha = 0.5, size = 1.2) +\rgeom_point(col = \u0026quot;black\u0026quot;, size = 1.5) +\rlabs(\rtitle = \u0026quot;Forecast Projection\u0026quot;,\ry = \u0026quot;Total Unique Views\u0026quot;\r) +\rtheme_minimal()\rAnd we also have actual data for that period. We have noticed an increase in total website visitors.\nactual %\u0026gt;% ggplot(mapping = aes(x = datetime, y = unique_views)) + geom_line(color = \u0026quot;steelblue\u0026quot;, size = 1.2, alpha = 0.6) +\rgeom_point(col = \u0026quot;black\u0026quot;, size = 1.5) +\rlabs(\rtitle = \u0026quot;Actual data on the number of website visitors\u0026quot;,\rsubtitle = \u0026quot;16 May to 17 July\u0026quot;,\ry = NULL\r) +\rtheme_minimal()\rTo estimate the causal effect, we begin by determining which period in the data should be used to train the model (the pre-intervention period) and which period to calculate the counterfactual prediction (the post-intervention period).\npre \u0026lt;- c(1,37)\rpost \u0026lt;- c(38, 53)\rThe code above means that the 1st to 37th observation points will be used for training, and the 38th to 53th observation points to calculate predictions, or we can also define them in the date interval format. Then change the data to a matrix format as a condition for analysis with the CausalImpact packages.\npre \u0026lt;- as.Date(c(\u0026quot;2018-05-16\u0026quot;, \u0026quot;2018-06-24\u0026quot;))\rpost \u0026lt;- as.Date(c(\u0026quot;2018-06-25\u0026quot;, \u0026quot;2018-07-10\u0026quot;))\rtime.points \u0026lt;- seq.Date(as.Date(\u0026quot;2018-05-16\u0026quot;), by = \u0026quot;days\u0026quot;, length.out = 53)\rdata_ci \u0026lt;- zoo(\rcbind(actual$unique_views, append_data$unique_views), time.points\r)\rNow we have the data ready to verify the causal effect of the campaign.\nimpact \u0026lt;- CausalImpact(data = data_ci, pre.period = pre, post.period = post)\rplot(impact)\rBy default, the plot contains three panels. The original first panel shows counter-factual data and predictions for the post campaign period. The second panel pointwise shows the difference between the actual observed (leads) and the predicted data. The third cumulative panel describes the cumulative effects of the intervention (campaign) conducted.\nThese results assume that the relationship between leads generation and the observed time series, as determined during the pre-period, remains stable throughout the post-period. We can view the statistical information by using the command summary(impact)\nsummary(impact)\r#\u0026gt; Posterior inference {CausalImpact}\r#\u0026gt; #\u0026gt; Average Cumulative #\u0026gt; Actual 1417 18424 #\u0026gt; Prediction (s.d.) 984 (55) 12786 (711) #\u0026gt; 95% CI [871, 1088] [11329, 14141]\r#\u0026gt; #\u0026gt; Absolute effect (s.d.) 434 (55) 5638 (711) #\u0026gt; 95% CI [329, 546] [4283, 7095] #\u0026gt; #\u0026gt; Relative effect (s.d.) 44% (5.6%) 44% (5.6%) #\u0026gt; 95% CI [33%, 55%] [33%, 55%] #\u0026gt; #\u0026gt; Posterior tail-area probability p: 0.00103\r#\u0026gt; Posterior prob. of a causal effect: 99.89669%\r#\u0026gt; #\u0026gt; For more details, type: summary(impact, \u0026quot;report\u0026quot;)\rWe can get information from the actual and predicted effect (average) and their absolute and relative effects. The output of statistical information above says, leads generation after the campaign has increased by 44%, from the estimated average website visitors of 984 people to 1417 in reality.\nFor guidance on correct interpretation of summary table results, the CausalImpact packages provide its interpretation text, which we can print using the command:\ninterpretation \u0026lt;- summary(impact, \u0026quot;report\u0026quot;)\r#\u0026gt; Analysis report {CausalImpact}\r#\u0026gt; #\u0026gt; #\u0026gt; During the post-intervention period, the response variable had an average value of approx. 1.42K. By contrast, in the absence of an intervention, we would have expected an average response of 0.98K. The 95% interval of this counterfactual prediction is [0.87K, 1.09K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 0.43K with a 95% interval of [0.33K, 0.55K]. For a discussion of the significance of this effect, see below.\r#\u0026gt; #\u0026gt; Summing up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 18.42K. By contrast, had the intervention not taken place, we would have expected a sum of 12.79K. The 95% interval of this prediction is [11.33K, 14.14K].\r#\u0026gt; #\u0026gt; The above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +44%. The 95% interval of this percentage is [+33%, +55%].\r#\u0026gt; #\u0026gt; This means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (0.43K) to the original goal of the underlying intervention.\r#\u0026gt; #\u0026gt; The probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant.\r\r","permalink":"http://ahmadhusain.in/post/causal-impact/","summary":"Background\rIn the business process, the marketing team has a role to play in increasing the brand awareness of a product. When the marketing team’s efforts are successful to get the customer’s attention to find out the product, it will generate a lead. In a simple way, leads are people who are interested in a business product.\nIn the digital era, leads can be interpreted as people who visit websites directly or through advertisements, people who like, share content or product campaigns.","title":"Causal Impact on Leads Generation"},{"content":"\r\rDeep Neural Network\rBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified. Simple Neural Network only has 1 hidden layer, whereas Deep Learning has more than 1 hidden layer.\nNerual Network architecture can also be characterized based on the direction of signal in training process: feed-forward and recurrent. In this material, we will discuss more the Recurrent Neural Network architecture.\nWe know that neural network uses an algorithm called Backpropagation to update the weights of the network. So what Backpropogation does is it first calculates the gradients from the error using the chain rule, then in updates the weights (Gradient Descent).\nWhen doing backpropogation in simple neural network (1 hidden layer) we might not encounter the problem update weights. But….\n\rWhen we build an architecture with a large number of hidden layer (Deep Neural Network) the model is likely to encounter update weight problem called vanishing / exploding gradient descent.\n\rVanishing / Exploding Gradient Descent\rVanishing Gradient Descent: the update value obtained will exponentially decrease when heading to the input layer. Here are the illustrations, which I took from: Michael Nguyen’s article\nGradient descent aims to adjust weights that allow the model to ‘learn’. the nature of the gradient that is, the greater the value of the gradient in the current layer, will affect in the next layer getting bigger. and vice versa. This is the problem. When doing BP, each node will calculate the gradient value and update its weight according to the gradient effect on the previous layer. so if the previous layer is small, then adjusting the weights in the current layer will be small. it causes the gradient to shrink exponentially when it goes to the input layer. so that when in the input layer it fails to do the learning due to vanishing gradient problems. so the model fails to learn when a pass forward is made again to make predictions.\nExploding Gradient Descent: the update value obtained will exponentially increase when heading to the input layer. The characteristics of the model have an exploding gradient problem, which is when the cost function results are NaN.\nHere is a link duscussion regarding to exploding grgadient descent: nan loss when training NN\n\r\rRecurrent Neural Network\rFrom the vasishing / exploding gradient problem mentioned above, the development of architecture from the RNN, namely LSTM and GRU, is able to handle the problem. will be discussed below. RNN itself has not been able to handle vanishing gradients due to short-term memory problems.\nSo what is RNN? RNN is a deep learning architecture model that is commonly used for sequential data. What’s the sequential data? The following are examples of sequential data cases:\nSentiment classification. Input: text, output: rating/sentiment class.\rTranslator. Input: text, output: text translator.\rTime series data: input: Numeric, output: forecasting result.\rSpeech Recognation: input: audio, output: text.\r\rRNN Concept\rThe main idea of RNN is to utilize sequential information processing. RNN is called repetitive because it performs the same task for each successive element, with output depending on the previous calculation.\nThe diagram above is the architecture of RNN after opening it unfolded. Suppose we have a sentence consisting of 10 words, it means that there will be 10 NN-layers formed separately. Each layer represents each word. The following are some explanations of the notation in the diagram above:\n\r\\(x_t\\) is an input at (time) \\(t\\).\rRNN stores an internal state \\(S_t\\) which becoming a memory in RNN. \\(S_t\\) is calculated based on the previous hidden state and the input in the current step. The activation function of \\(f\\) is usually nonlinear like tanh or ReLU.\r\r\r\\(S_t = f(U_{xt} + W_{St} − 1)\\).\r\r\n\r\\(O_t\\) is an output from each step \\(t\\)\r\rForward Pass\rFor example case studies related to sentiment classification. Then the input will contain one-hot vectors of words, and the output is a sentiment class. So a feed-forward pass scheme is performed as:\nEach layer describes each word.\rFor an explanation of the concept of the Forward pass, we look at the layer when \\(t\\) (in the middle).\r\r\rRemember that the RNN pays attention to the output calculation data at \\(t - 1\\). So initially we have to calculate the state at \\(S_t\\) first. Involving the multiplication of the input matrix \\(x_t\\) with the parameter \\(U\\) and summing the results of the product \\(s_{t1}\\). Then the results are processed with the tanh activation function. The calculation details are as follows:\r\r\r\\(s_t=tanh(U.x_t+W.s_{t−1)}\\)\r\r\n\rThe results from \\(s_t\\) are then passed to the output by tiling the matrix multiplication of the \\(V\\) parameters and then passed with the softmax activation function. The details are as follows:\r\r\r\\(\\hat{y}_t=softmax(V.s_t)\\)\r\r\nThe above process is often illustrated like:\n\rBackpropogation Through Time\rThe purpose of the RNN model training is to find the parameters \\(U, V,\\) and \\(W\\) which produce minimum errors. The term BTT arises because the RNN architecture pays attention to the previous time series. So to calculate the gradient at the time step \\(t\\), we must calculate the speech at step \\(t − 1, t − 2, t − 3\\) until it is at time \\(t = 1\\). If you are curious about how this algorithm works, I suggest to read more on this article: Part 3: Backpropogarion \u0026amp; Vanishing Gradient.\nAs mentioned in the previous chapter, the RNN model also has a vanishing gradient problem, because it cannot capture long-term dependencies. because the number of layers is too long, making the backprop process produces a gradient value that is getting smaller and even close to zero or is said to disappear when it arrives at the initial layers.\n\rThis is caused by the multiplication properties between fractions. Imagine that for example a fraction is 1/4, multiplied by another fraction such as 1/3, then in one operation the value is 1/12. Multiplied by other fractions such as 1/5, the value becomes 1/60, etc. This value will shrink exponentially, and with a small fractional value and many multiplication operations, the value will be close to zero.\n\rTo overcome this problem, there is a development of RNN model namely Long-Term Short Memory (LSTM).\n\r\r\rLSTM\rJust like RNN, LSTM has a sequential model which is illustrated with a green box. if unfolded the architecture becomes as below:\nThe difference between RNN and LSTM is that it has additional signal information that is given from one time step to the next time step which is commonly called “cell memory”. LSTM is designed to overcome the problem of vanishing gradient, using the gate mechanism.\nLSTM Network\rSo the components in LSTM consist of:\n\rForget Gate f (NN with sigmoid as activation function).\rCandidate Layer g (NN with tanh as activation function).\rInput Gate I (NN with sigmoid as activation function).\rOutput Gate O (NN with sigmoid as activation function).\rHidden State H (vector).\rMemory State C (vector).\r\rThe following is the LSTM diagram at the t-time step.\n\r\\(X_t\\) = Input vector at the t-time.\r\\(H_{t−1}\\) = Previous Hidden state.\r\\(C_{t−1}\\) = Previous Memory state.\r\\(H_t\\) = Current Hidden state.\r\\(C_t\\) = Current Memori state.\r[*] = multiplication operation.\r[+] = addition operation.\r\rso the input of each LSTM module is \\(X_t\\) (current input), \\(H_{t − 1}\\), and \\(C_{t − 1}\\). then the output is \\(H_t\\), and \\(C_t\\).\nextracted from Denny Britz’s article which is a summary of Christopher Olah’s article: Understanding LSTM Networks.\n\\(l, f, O\\) is the Input, Forget, and Output gates. Both input, forget, and output have the same function formula (sigmoid), which only distinguishes the matrix parameters (note the formula below). This means that the output of this gate has a vector of values between 0 to 1. zero means that the information is blocked completely, and one means that all information is included. The gate input controls how many states you have just computed for the current input that you want to let pass. The forget gate controls how many previous states you want to let pass. Finally, the gate output controls how many internal states you want to expose to the network (higher layer \u0026amp; next time step). All gates have the same dimensions as the hidden state dimension (etc.) as a measure for the hidden state. The output of the sigmoid gate will be multiplied by another value to control how much that value is used.\r\r\r\\(I= \\sigma(x_tU^I + s_{t-_1}W^I)\\) \\(f= \\sigma(x_tU^f + s_{t-_1}W^f)\\) \\(O= \\sigma(x_tU^O + s_{t-_1}W^O)\\) \r\rvalue 1 means “really take care of this element” while 0 means “completely get rid of this element”.\n\r\\(g\\) is a “candidate” hidden state that is computed based on the current input and the previous hidden state.\n\r\\(c_t\\) is the internal memory of the unit. It is a combination of the previous memory \\(c_{t-1}\\) multiplied by the forget gate, and the newly computed hidden state g, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input.\n\r\r\r\\(c_t=f_t*c_{t-_1} + I_t*g\\)\r\r\r\rImplementation LSTM with Keras\r# load packages required\rlibrary(keras)\rlibrary(RVerbalExpressions)\rlibrary(magrittr)\rlibrary(textclean)\rlibrary(tidyverse)\rlibrary(tidytext)\rlibrary(rsample)\rlibrary(yardstick)\rlibrary(caret)\r# set conda env\ruse_condaenv(\u0026quot;tensorflow\u0026quot;)\rImport Data\rIn this example we will use a case study of sentiment tweets about airlines in the US obtained from Kaggle.\ndata \u0026lt;- read_csv(\u0026quot;data_input/tweets.csv\u0026quot;)\rglimpse(data)\r## Rows: 14,640\r## Columns: 15\r## $ tweet_id \u0026lt;dbl\u0026gt; 5.703061e+17, 5.703011e+17, 5.703011e+...\r## $ airline_sentiment \u0026lt;chr\u0026gt; \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;neg...\r## $ airline_sentiment_confidence \u0026lt;dbl\u0026gt; 1.0000, 0.3486, 0.6837, 1.0000, 1.0000...\r## $ negativereason \u0026lt;chr\u0026gt; NA, NA, NA, \u0026quot;Bad Flight\u0026quot;, \u0026quot;Can\u0026#39;t Tell\u0026quot;...\r## $ negativereason_confidence \u0026lt;dbl\u0026gt; NA, 0.0000, NA, 0.7033, 1.0000, 0.6842...\r## $ airline \u0026lt;chr\u0026gt; \u0026quot;Virgin America\u0026quot;, \u0026quot;Virgin America\u0026quot;, \u0026quot;V...\r## $ airline_sentiment_gold \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ name \u0026lt;chr\u0026gt; \u0026quot;cairdin\u0026quot;, \u0026quot;jnardino\u0026quot;, \u0026quot;yvonnalynn\u0026quot;, \u0026quot;...\r## $ negativereason_gold \u0026lt;lgl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ retweet_count \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r## $ text \u0026lt;chr\u0026gt; \u0026quot;@VirginAmerica What @dhepburn said.\u0026quot;,...\r## $ tweet_coord \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...\r## $ tweet_created \u0026lt;chr\u0026gt; \u0026quot;2015-02-24 11:35:52 -0800\u0026quot;, \u0026quot;2015-02-...\r## $ tweet_location \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Lets Play\u0026quot;, NA, NA, NA, \u0026quot;San ...\r## $ user_timezone \u0026lt;chr\u0026gt; \u0026quot;Eastern Time (US \u0026amp; Canada)\u0026quot;, \u0026quot;Pacific...\r\rText Pre-Processing\rSetup regex\rmention\rmention \u0026lt;- rx() %\u0026gt;% rx_find(value = \u0026quot;@\u0026quot;) %\u0026gt;% rx_alnum() %\u0026gt;% rx_one_or_more()\rmention\r## [1] \u0026quot;(@)[A-z0-9]+\u0026quot;\r\u0026quot;@VirginAmerica What @dhepburn said.\u0026quot; %\u0026gt;% str_remove_all(pattern = mention) %\u0026gt;% str_squish()\r## [1] \u0026quot;What said.\u0026quot;\r\rhashtag\rhashtag \u0026lt;- rx() %\u0026gt;% rx_find(value = \u0026quot;#\u0026quot;) %\u0026gt;% rx_alnum() %\u0026gt;% rx_one_or_more()\rhashtag\r## [1] \u0026quot;(#)[A-z0-9]+\u0026quot;\r\u0026quot;@VirginAmerica I\u0026#39;m #elevategold for a good reason: you rock!!\u0026quot; %\u0026gt;% str_remove_all(pattern = mention) %\u0026gt;%\rstr_remove_all(pattern = hashtag) %\u0026gt;% str_squish()\r## [1] \u0026quot;I\u0026#39;m for a good reason: you rock!!\u0026quot;\r\rquestion mark\rquestion \u0026lt;- rx() %\u0026gt;% rx_find(value = \u0026quot;?\u0026quot;) %\u0026gt;% rx_one_or_more()\rquestion\r## [1] \u0026quot;(\\\\?)+\u0026quot;\r\rexclamation mark\rexclamation \u0026lt;- rx() %\u0026gt;% rx_find(value = \u0026quot;!\u0026quot;) %\u0026gt;% rx_one_or_more()\rexclamation\r## [1] \u0026quot;(!)+\u0026quot;\r\rpunctuation\rpunctuation \u0026lt;- rx_punctuation()\rpunctuation\r## [1] \u0026quot;[[:punct:]]\u0026quot;\r\rnumber\rnumber \u0026lt;- rx_digit()\rnumber\r## [1] \u0026quot;\\\\d\u0026quot;\r\rdollar sign\rdollar \u0026lt;- rx() %\u0026gt;% rx_find(\u0026quot;$\u0026quot;)\rdollar\r## [1] \u0026quot;(\\\\$)\u0026quot;\r\r\rText Cleansing\rreplace_url\r\u0026quot;@VirginAmerica Really missed a prime opportunity, there. https://t.co/mWpG7grEZP\u0026quot; %\u0026gt;% replace_url()\r## [1] \u0026quot;@VirginAmerica Really missed a prime opportunity, there. \u0026quot;\r\rreplace_emoticon\r\u0026quot;@SouthwestAir thanks! Very excited to see it :3\u0026quot; %\u0026gt;%\rreplace_emoticon()\r## [1] \u0026quot;@SouthwestAir thanks! Very excited to see it smiley \u0026quot;\r\rreplace_contruction\r\u0026quot;@united I\u0026#39;d thank you - but you didn\u0026#39;t help. taking 6 hours to reply isn\u0026#39;t actually helpful\u0026quot; %\u0026gt;% replace_contraction()\r## [1] \u0026quot;@united I would thank you - but you did not help. taking 6 hours to reply is not actually helpful\u0026quot;\r\rreplace_word_elongation\r\u0026quot;@VirginAmerica heyyyy guyyyys.. :/\u0026quot; %\u0026gt;% replace_word_elongation()\r## [1] \u0026quot;@VirginAmerica hey guys.. :/\u0026quot;\rdata \u0026lt;- data %\u0026gt;% mutate(\rtext_clean = text %\u0026gt;% replace_url() %\u0026gt;% replace_emoji() %\u0026gt;% replace_emoticon() %\u0026gt;% replace_html() %\u0026gt;% str_remove_all(pattern = mention) %\u0026gt;% str_remove_all(pattern = hashtag) %\u0026gt;% replace_contraction() %\u0026gt;% replace_word_elongation() %\u0026gt;% str_replace_all(pattern = question, replacement = \u0026quot;questionmark\u0026quot;) %\u0026gt;% str_replace_all(pattern = exclamation, replacement = \u0026quot;exclamationmark\u0026quot;) %\u0026gt;% str_remove_all(pattern = punctuation) %\u0026gt;% str_remove_all(pattern = number) %\u0026gt;% str_remove_all(pattern = dollar) %\u0026gt;% str_to_lower() %\u0026gt;% str_squish()\r)\rdata %\u0026gt;% select(text, text_clean) %\u0026gt;% sample_n(20)\r## # A tibble: 20 x 2\r## text text_clean ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026quot;@SouthwestAir and this asshole cut ~ and this asshole cut in front of me yo~\r## 2 \u0026quot;@united DUH and done. bag whereabou~ \u0026lt;NA\u0026gt; ## 3 \u0026quot;@SouthwestAir great flight! And gre~ great flightexclamationmark and great ~\r## 4 \u0026quot;@SouthwestAir kids on a mission tri~ kids on a mission trip from lasalle st~\r## 5 \u0026quot;@USAirways my sister was supposed t~ my sister was supposed to leave at am ~\r## 6 \u0026quot;@AmericanAir already did \u0026amp;amp; an a~ already did an automated voice told us~\r## 7 \u0026quot;@united you suck. 9 hour delay?!\u0026quot; you suck hour delayquestionmarkexclama~\r## 8 \u0026quot;@JetBlue don\u0026#39;t think any1 rly knows~ do not think any rly knows what is goi~\r## 9 \u0026quot;@AmericanAir no no no, YOU DM me, I~ no no no you dm me i pay you money and~\r## 10 \u0026quot;@SouthwestAir Just OH-ing a text me~ just ohing a text message i received f~\r## 11 \u0026quot;@AmericanAir I was flying from Ft L~ i was flying from ft lauderdale fl to ~\r## 12 \u0026quot;@AmericanAir thanks for the great c~ thanks for the great customer service ~\r## 13 \u0026quot;@VirginAmerica no A\u0026#39;s channel this ~ no as channel this yearquestionmark ## 14 \u0026quot;@AmericanAir Is there a way I can g~ is there a way i can get a record of a~\r## 15 \u0026quot;@VirginAmerica Comenity Bank is a j~ comenity bank is a jokeexclamationmark~\r## 16 \u0026quot;@united message sent. Thank you!\u0026quot; message sent thank youexclamationmark ## 17 \u0026quot;@united hey! think someone could me~ heyexclamationmark think someone could~\r## 18 \u0026quot;@USAirways it\u0026#39;s painful being on ho~ it is painful being on hold to youexcl~\r## 19 \u0026quot;@united the lack of customer servic~ the lack of customer service is astoun~\r## 20 \u0026quot;@AmericanAir Tired of sitting on a ~ tired of sitting on a delayed again an~\r\r\rprepare datainput\rdata \u0026lt;- data %\u0026gt;% mutate(label = factor(airline_sentiment, levels = c(\u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;)),\rlabel = as.numeric(label),\rlabel = label - 1) %\u0026gt;% select(text_clean, label) %\u0026gt;% na.omit()\rhead(data, 10)\r## # A tibble: 10 x 2\r## text_clean label\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 what said 1\r## 2 plus you have added commercials to the e tongue sticking out erience t~ 2\r## 3 i did not today must mean i need to take another tripexclamationmark 1\r## 4 it is really aggressive to blast obnoxious entertainment in your guest~ 0\r## 5 and it is a really big bad thing about it 0\r## 6 seriously would pay a flight for seats that did not have this playing ~ 0\r## 7 yes nearly every time i fly vx this ear worm won t go away smiley 2\r## 8 really missed a prime opportunity for men without hats parody there 1\r## 9 well i did not but now i doexclamationmark laughing 2\r## 10 it was amazing and arrived an hour early you are too good to me 2\r\r\rTokenizer\rTokenizer aims to separate each word in the entire document into a token form. The num_words parameter is for setting the maximum number of words to be used, sorted according to the largest frequency order. words that rarely appear will be removed. from a total of 13291 unique words contained in the text data, we reduced it to 1024 which will be used to make the model. The lower parameter is a logic condition, if TRUE then all words will be transformed to lowercase (tolower).\nnum_words \u0026lt;- 1024 # prepare tokenizers\rtokenizer \u0026lt;- text_tokenizer(num_words = num_words,\rlower = TRUE) %\u0026gt;% fit_text_tokenizer(data$text_clean)\rpaste(\u0026quot;number of unique words:\u0026quot;, length(tokenizer$word_counts))\r## [1] \u0026quot;number of unique words: 13291\u0026quot;\rIntuition\rSuppose we have 5 pieces of text documents that are stored in the docs object. Then we made a token with a maximum of words / terms used, which is 4. It means that words that rarely appear will not be used during the train model. To see the number of unique words stored in the document dictionary, use the command token$word_counts. To see the list of words with the highest frequency of appearances, use the token$word_index command.\ndocs \u0026lt;- c(\u0026#39;Well done!\u0026#39;,\r\u0026#39;Good work\u0026#39;,\r\u0026#39;Great effort\u0026#39;,\r\u0026#39;nice work\u0026#39;,\r\u0026#39;Excellent!\u0026#39;)\rtokendocs \u0026lt;- text_tokenizer(num_words = 4, lower = TRUE) %\u0026gt;% fit_text_tokenizer(docs)\rpaste(\u0026quot;number of unique words\u0026quot;,length(tokendocs$word_counts))\r## [1] \u0026quot;number of unique words 8\u0026quot;\rtokendocs$word_index[1:4]\r## $work\r## [1] 1\r## ## $well\r## [1] 2\r## ## $done\r## [1] 3\r## ## $good\r## [1] 4\r\r\rSplitting Data\rSplitting data will be done into 3 parts, namely train, validation, and test. The proportion is 60% for trains and the remaining 40% is in partitions for data validation and testing.\nData Train is the data that we will use to train the model. Data Validation for evaluating hyperparameter tuning in models (adjust hidden layers, optimizers, learning rates, etc.). While the test data as an evaluator of the model that we make on unseen data.\nset.seed(100)\rintrain \u0026lt;- initial_split(data = data, prop = 0.8, strata = \u0026quot;label\u0026quot;)\rdata_train \u0026lt;- training(intrain)\rdata_test \u0026lt;- testing(intrain)\rset.seed(100)\rinval \u0026lt;- initial_split(data = data_test, prop = 0.5, strata = \u0026quot;label\u0026quot;)\rdata_val \u0026lt;- training(inval)\rdata_test \u0026lt;- testing(inval)\rmaxlen \u0026lt;- max(str_count(data$text_clean, \u0026quot;\\\\w+\u0026quot;)) + 1 paste(\u0026quot;maxiumum length words in data:\u0026quot;, maxlen)\r## [1] \u0026quot;maxiumum length words in data: 90\u0026quot;\r# prepare x\rdata_train_x \u0026lt;- texts_to_sequences(tokenizer, data_train$text_clean) %\u0026gt;%\rpad_sequences(maxlen = maxlen)\rdata_val_x \u0026lt;- texts_to_sequences(tokenizer, data_val$text_clean) %\u0026gt;%\rpad_sequences(maxlen = maxlen)\rdata_test_x \u0026lt;- texts_to_sequences(tokenizer, data_test$text_clean) %\u0026gt;%\rpad_sequences(maxlen = maxlen)\r# prepare y\rdata_train_y \u0026lt;- to_categorical(data_train$label, num_classes = 3)\rdata_val_y \u0026lt;- to_categorical(data_val$label, num_classes = 3)\rdata_test_y \u0026lt;- to_categorical(data_test$label, num_classes = 3)\rIntuition\rCommand texts_to_sequence() aims to create a matrix results of the transformation text to the form of a number sequence (integer). Then wrapped with the command pad_sequences() which aims to equalize the dimensions of the length on the entire document. Imagine the input layer of a matrix, it must have the same row and column. Therefore it is necessary to do padding. By default the value parameter will be set to 0. This means that if there are words that are not in our token (which has been limited by num_words) then it will be transformed to 0. The following is the illustration:\ntexts_to_sequences(tokendocs, c(\u0026quot;Excellent!\u0026quot;, \u0026quot;Good job bro, keep hard work\u0026quot;, \u0026quot;well done\u0026quot;)) %\u0026gt;% pad_sequences(maxlen = 5)\r## [,1] [,2] [,3] [,4] [,5]\r## [1,] 0 0 0 0 0\r## [2,] 0 0 0 0 1\r## [3,] 0 0 0 2 3\rThe result of text_to_sequences is a matrix of size \\(n ∗ maxlen\\). The example above consists of 3 text documents and is set to maxlen = 5. it will produce a \\(3 \\times 5\\) matrix which each index is a representative integer of the same words as tokendocs in the i-th list. Recall, the word done is on the 3rd list on our token, right? therefore the matrix result above in the third document and the last integer sequence is 3. Why does it appear in the last index? because in pad_sequences we don’t set parameters of the padding type whether “pre” or “post” and by default is “pre”.\ntokendocs$word_index[3]\r## $done\r## [1] 3\r\r\rArchitecture\rEmbedding Layer\rEmbedding Layers can only be used in the initial / first layer of the LSTM architecture. In a variety of deep learning frameworks such as Keras, the embedding layer aims to train text data into numerical vectors which represent the closeness of the meaning of each word.\nEmbedding layer accepts several parameters. Some examples are:\r* input_dim, which is the maximum dimension of the vocabulary that has been explained in the num_words section.\n\rinput_length, the maximum length of the word sequence in the document input.\n\routput_dim which is the embedding dimension of the output layer which will be passed to the next layer. generally is 32, but can be more dependent on the problem we face.\n\r\r\rInput received of 2D vectors with the form: {batch_size, sequence_length}, while the output received 3D tensor with the forms {batch_size, sequence_length, output_dim}.\n\r\rDeep Neural Layer\rThe Deep Network Layer accepts the embedding matrix as input and then is converted into smaller dimensions. The dimensions of the compression results have represented information from the data. In the case of data text, the deep learning architecture commonly used is RNN \u0026gt; LSTM / GRU.\n\ryou can check the Keras Documentation for the details sequential layers.\n\r\rOutput Layer\rThis output layer is the last layer in the deep learning architecture. At Keras use the layer_dense command where we need to set the unit parameters or how many neurons we want to build. In this case I use 3 units, because there are 3 classes we have (negative, neutral, positive).\n\rRandom Initialization\rWhen the neural network / deep learning model train often results in different results. Why? because NN and DL use weigth which is generated randomly (randomness initialization). therefore we need to set the numbers (x-random models) in order to get a fixed result when repeated in the train (reproducible result). this can be done with the seed parameter in the initializer_random_uniform command. for more details, read the question and answer article in Keras studio\n# initiate keras model sequence\rmodel \u0026lt;- keras_model_sequential()\r# model\rmodel %\u0026gt;%\r# layer input\rlayer_embedding(\rname = \u0026quot;input\u0026quot;,\rinput_dim = num_words,\rinput_length = maxlen,\routput_dim = 32, embeddings_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r) %\u0026gt;%\r# layer dropout\rlayer_dropout(\rname = \u0026quot;embedding_dropout\u0026quot;,\rrate = 0.5\r) %\u0026gt;%\r# layer lstm 1\rlayer_lstm(\rname = \u0026quot;lstm\u0026quot;,\runits = 256,\rdropout = 0.2,\rrecurrent_dropout = 0.2,\rreturn_sequences = FALSE, recurrent_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2),\rkernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r) %\u0026gt;%\r# layer output\rlayer_dense(\rname = \u0026quot;output\u0026quot;,\runits = 3,\ractivation = \u0026quot;softmax\u0026quot;, kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r)\r\rDropout parameters are added to reduce the risk of overfit. the range of dropout values between 0 to 1. commonly used is 0.2 to 0.5. the closer to 0 will tend to overfit, while the closer to 1 has the risk of underfit.\r\r\rCompiling Model\rFor two category classes, the lost function used is binary_crossentropy while for multiclass cases it uses categorical_crossentropy. There are not only 2 option, but the most common when working with classification cases, these 2 loss functions are used. Here are some loss function options from Keras Documentation\n# compile the model\rmodel %\u0026gt;% compile(\roptimizer = \u0026quot;adam\u0026quot;,\rmetrics = \u0026quot;accuracy\u0026quot;,\rloss = \u0026quot;categorical_crossentropy\u0026quot;\r)\r# model summary\rsummary(model)\r## Model: \u0026quot;sequential\u0026quot;\r## ________________________________________________________________________________\r## Layer (type) Output Shape Param # ## ================================================================================\r## input (Embedding) (None, 90, 32) 32768 ## ________________________________________________________________________________\r## embedding_dropout (Dropout) (None, 90, 32) 0 ## ________________________________________________________________________________\r## lstm (LSTM) (None, 256) 295936 ## ________________________________________________________________________________\r## output (Dense) (None, 3) 771 ## ================================================================================\r## Total params: 329,475\r## Trainable params: 329,475\r## Non-trainable params: 0\r## ________________________________________________________________________________\r\r\rTrain the Model\r# model fit settings\repochs \u0026lt;- 10\rbatch_size \u0026lt;- 512\r# fit the model\rhistory \u0026lt;- model %\u0026gt;% fit(\rdata_train_x, data_train_y,\rbatch_size = batch_size, epochs = epochs,\rverbose = 1,\rvalidation_data = list(\rdata_val_x, data_val_y\r)\r)\r# history plot\rplot(history)\r\rModel Evaluation\r# predict on train\rdata_train_pred \u0026lt;- model %\u0026gt;%\rpredict_classes(data_train_x) %\u0026gt;%\ras.vector()\r# predict on val\rdata_val_pred \u0026lt;- model %\u0026gt;%\rpredict_classes(data_val_x) %\u0026gt;%\ras.vector()\r# predict on test\rdata_test_pred \u0026lt;- model %\u0026gt;%\rpredict_classes(data_test_x) %\u0026gt;%\ras.vector()\r# accuracy on data train\raccuracy_vec(\rtruth = factor(data_train$label,labels = c(\u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;)),\restimate = factor(data_train_pred, labels = c(\u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;))\r)\r## [1] 0.8011857\r# accuracy on data test\raccuracy_vec(\rtruth = factor(data_test$label,labels = c(\u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;)),\restimate = factor(data_test_pred, labels = c(\u0026quot;negative\u0026quot;, \u0026quot;neutral\u0026quot;, \u0026quot;positive\u0026quot;))\r)\r## [1] 0.7744154\r\r\r","permalink":"http://ahmadhusain.in/post/2018-09-21-text-classification-with-lstm/","summary":"Deep Neural Network\rBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer.","title":"Text Classification with LSTM"},{"content":"\r\rHere is a tutorial how to replicate an economist publication plot using ggplot2. Please prepare the library required first and download the data on the link below to follow the code.\nlink for download the report: https://www2.deloitte.com/content/dam/Deloitte/cr/Documents/public-sector/2014-Social-Progress-IndexRepIMP.pdf\nLoad Library\rlibrary(tidyverse)\rlibrary(pdftools)\rlibrary(ggthemes)\rlibrary(ggpubr)\rlibrary(tm)\rlibrary(grid)\rlibrary(ggrepel)\rlibrary(scales)\r\rImport Data\rdatatext \u0026lt;- pdf_text(\u0026quot;data_input/2014-Social-Progress-IndexRepIMP.pdf\u0026quot;)\r\rData Pre-Processing\rdatatext88 \u0026lt;- datatext[[88]] %\u0026gt;%\rread_lines()\rdata1 \u0026lt;- datatext88[7:50]\rhead(data1)\r#\u0026gt; [1] \u0026quot; 1 New Zealand $25,857 88.24 91.74 84.97 88.01 97.57 100.00\u0026quot;\r#\u0026gt; [2] \u0026quot; 2 Switzerland $39,293 88.19 94.87 89.78 79.92 98.33 99.92\u0026quot;\r#\u0026gt; [3] \u0026quot; 3 Iceland $33,880 88.07 94.32 88.19 81.71 98.78 100.00\u0026quot;\r#\u0026gt; [4] \u0026quot; 4 Netherlands $36,438 87.37 93.91 87.56 80.63 98.16 100.00\u0026quot;\r#\u0026gt; [5] \u0026quot; 5 Norway $47,547 87.12 93.59 86.94 80.82 98.71 100.00\u0026quot;\r#\u0026gt; [6] \u0026quot; 6 Sweden $34,945 87.08 94.59 84.71 81.95 98.26 100.00\u0026quot;\rall_data1_lines \u0026lt;- data1[1:44] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;)\rhead(all_data1_lines)\r#\u0026gt; [1] \u0026quot;1 New Zealand $25857 88.24 91.74 84.97 88.01 97.57 100.00\u0026quot;\r#\u0026gt; [2] \u0026quot;2 Switzerland $39293 88.19 94.87 89.78 79.92 98.33 99.92\u0026quot; #\u0026gt; [3] \u0026quot;3 Iceland $33880 88.07 94.32 88.19 81.71 98.78 100.00\u0026quot; #\u0026gt; [4] \u0026quot;4 Netherlands $36438 87.37 93.91 87.56 80.63 98.16 100.00\u0026quot;\r#\u0026gt; [5] \u0026quot;5 Norway $47547 87.12 93.59 86.94 80.82 98.71 100.00\u0026quot; #\u0026gt; [6] \u0026quot;6 Sweden $34945 87.08 94.59 84.71 81.95 98.26 100.00\u0026quot;\rall_data1_lines_sub \u0026lt;- data1[-c(1, 13, 16, 23,25,28,37)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;)\rhead(all_data1_lines_sub)\r#\u0026gt; [[1]]\r#\u0026gt; [1] \u0026quot;2\u0026quot; \u0026quot;Switzerland\u0026quot; \u0026quot;$39293\u0026quot; \u0026quot;88.19\u0026quot; \u0026quot;94.87\u0026quot; #\u0026gt; [6] \u0026quot;89.78\u0026quot; \u0026quot;79.92\u0026quot; \u0026quot;98.33\u0026quot; \u0026quot;99.92\u0026quot; #\u0026gt; #\u0026gt; [[2]]\r#\u0026gt; [1] \u0026quot;3\u0026quot; \u0026quot;Iceland\u0026quot; \u0026quot;$33880\u0026quot; \u0026quot;88.07\u0026quot; \u0026quot;94.32\u0026quot; \u0026quot;88.19\u0026quot; \u0026quot;81.71\u0026quot; #\u0026gt; [8] \u0026quot;98.78\u0026quot; \u0026quot;100.00\u0026quot; #\u0026gt; #\u0026gt; [[3]]\r#\u0026gt; [1] \u0026quot;4\u0026quot; \u0026quot;Netherlands\u0026quot; \u0026quot;$36438\u0026quot; \u0026quot;87.37\u0026quot; \u0026quot;93.91\u0026quot; #\u0026gt; [6] \u0026quot;87.56\u0026quot; \u0026quot;80.63\u0026quot; \u0026quot;98.16\u0026quot; \u0026quot;100.00\u0026quot; #\u0026gt; #\u0026gt; [[4]]\r#\u0026gt; [1] \u0026quot;5\u0026quot; \u0026quot;Norway\u0026quot; \u0026quot;$47547\u0026quot; \u0026quot;87.12\u0026quot; \u0026quot;93.59\u0026quot; \u0026quot;86.94\u0026quot; \u0026quot;80.82\u0026quot; \u0026quot;98.71\u0026quot; #\u0026gt; [9] \u0026quot;100.00\u0026quot;\r#\u0026gt; #\u0026gt; [[5]]\r#\u0026gt; [1] \u0026quot;6\u0026quot; \u0026quot;Sweden\u0026quot; \u0026quot;$34945\u0026quot; \u0026quot;87.08\u0026quot; \u0026quot;94.59\u0026quot; \u0026quot;84.71\u0026quot; \u0026quot;81.95\u0026quot; \u0026quot;98.26\u0026quot; #\u0026gt; [9] \u0026quot;100.00\u0026quot;\r#\u0026gt; #\u0026gt; [[6]]\r#\u0026gt; [1] \u0026quot;7\u0026quot; \u0026quot;Canada\u0026quot; \u0026quot;$35936\u0026quot; \u0026quot;86.95\u0026quot; \u0026quot;93.52\u0026quot; \u0026quot;80.31\u0026quot; \u0026quot;87.02\u0026quot; \u0026quot;98.10\u0026quot; #\u0026gt; [9] \u0026quot;95.76\u0026quot;\rdatadf88_sub \u0026lt;- plyr::ldply(all_data1_lines_sub) %\u0026gt;%\rselect(c(V2,V3,V4)) %\u0026gt;%\rrename(Country = V2,\rGDP = V3,\rSPI = V4)\rdatadf88_sub_2 \u0026lt;- data1[c(1,13,16,23,25)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3)) %\u0026gt;%\rselect(c(V2,V4,V5)) %\u0026gt;%\rrename(Country = V2,\rGDP = V4,\rSPI = V5)\rdatadf88_sub_3 \u0026lt;- data1[c(28,37)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3,V4)) %\u0026gt;%\rselect(c(V2,V5,V6)) %\u0026gt;%\rrename(Country = V2,\rGDP = V5,\rSPI = V6)\rdf88 \u0026lt;- rbind(datadf88_sub,datadf88_sub_2,datadf88_sub_3)\rdf88$GDP \u0026lt;- str_remove(df88$GDP, \u0026quot;[$]\u0026quot;)\rdf88 \u0026lt;- df88 %\u0026gt;%\rmutate(GDP = as.numeric(GDP),\rSPI = as.numeric(SPI))\r#Next page\rdatatext90 \u0026lt;- datatext[[90]] %\u0026gt;%\rread_lines()\rdata2 \u0026lt;- datatext90[7:50]\rall_data2_lines \u0026lt;- data2[1:44] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;)\rall_data2_lines_sub \u0026lt;- data2[-c(3, 17, 19, 21, 24, 25, 41)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;)\rdatadf90_sub \u0026lt;- plyr::ldply(all_data2_lines_sub) %\u0026gt;%\rselect(c(V2,V3,V4)) %\u0026gt;%\rrename(Country = V2,\rGDP = V3,\rSPI = V4)\rdatadf90_sub_2 \u0026lt;- data2[c(19, 21, 24, 25, 41)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3)) %\u0026gt;%\rselect(c(V2,V4,V5)) %\u0026gt;%\rrename(Country = V2,\rGDP = V4,\rSPI = V5)\rdatadf90_sub_3 \u0026lt;- data2[c(3,17)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3,V4)) %\u0026gt;%\rselect(c(V2,V5,V6)) %\u0026gt;%\rrename(Country = V2,\rGDP = V5,\rSPI = V6)\rdf90 \u0026lt;- rbind(datadf90_sub,datadf90_sub_2,datadf90_sub_3)\rdf90$GDP \u0026lt;- str_remove(df90$GDP, \u0026quot;[$]\u0026quot;)\rdf90 \u0026lt;- df90 %\u0026gt;%\rmutate(GDP = as.numeric(GDP),\rSPI = as.numeric(SPI))\r#The last Page\rdatatext92 \u0026lt;- datatext[[92]] %\u0026gt;%\rread_lines()\rdata3 \u0026lt;- datatext92[7:50]\rall_data3_lines \u0026lt;- data3[1:44] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;)\rall_data3_lines_sub \u0026lt;- data3[-c(22, 24, 43)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;)\rdatadf92_sub \u0026lt;- plyr::ldply(all_data3_lines_sub) %\u0026gt;%\rselect(c(V2,V3,V4)) %\u0026gt;%\rrename(Country = V2,\rGDP = V3,\rSPI = V4)\rdatadf92_sub_2 \u0026lt;- data3[24] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3)) %\u0026gt;%\rselect(c(V2,V4,V5)) %\u0026gt;%\rrename(Country = V2,\rGDP = V4,\rSPI = V5)\rdatadf92_sub_3 \u0026lt;- data3[c(22,43)] %\u0026gt;%\rstr_squish() %\u0026gt;%\rstr_replace_all(\u0026quot;,\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstrsplit(split = \u0026quot; \u0026quot;) %\u0026gt;%\rplyr::ldply() %\u0026gt;%\rmutate(V2 = paste(V2,V3,V4)) %\u0026gt;%\rselect(c(V2,V5,V6)) %\u0026gt;%\rrename(Country = V2,\rGDP = V5,\rSPI = V6)\rdf92 \u0026lt;- rbind(datadf92_sub,datadf92_sub_2,datadf92_sub_3)\rdf92$GDP \u0026lt;- str_remove(df92$GDP, \u0026quot;[$]\u0026quot;)\rdf92 \u0026lt;- df92 %\u0026gt;%\rmutate(GDP = as.numeric(GDP),\rSPI = as.numeric(SPI))\r#combine all data\rdata \u0026lt;- rbind(df88, df90, df92)\rdata \u0026lt;- data %\u0026gt;% arrange(desc(SPI)) %\u0026gt;%\rmutate(Country = ifelse(Country == \u0026quot;United Kingdom\u0026quot;, \u0026quot;Britain\u0026quot;, Country)) %\u0026gt;%\rmutate(Col_Country = ifelse(Country == \u0026quot;Costa Rica\u0026quot; | Country == \u0026quot;Brazil\u0026quot; | Country == \u0026quot;China\u0026quot; | Country == \u0026quot;Chad\u0026quot; | Country == \u0026quot;Angola\u0026quot; | Country == \u0026quot;India\u0026quot; |\rCountry == \u0026quot;Iran\u0026quot; | Country == \u0026quot;Philippines\u0026quot; | Country == \u0026quot;Jamaica\u0026quot; | Country == \u0026quot;Russia\u0026quot; | Country == \u0026quot;Greece\u0026quot;| Country == \u0026quot;Italy\u0026quot; | Country == \u0026quot;Israel\u0026quot; | Country == \u0026quot;Saudi Arabia\u0026quot; | Country == \u0026quot;France\u0026quot; |\rCountry == \u0026quot;Korea\u0026quot; | Country == \u0026quot;Japan\u0026quot; | Country == \u0026quot;Britain\u0026quot; | Country == \u0026quot;Germany\u0026quot; |\rCountry == \u0026quot;Canada\u0026quot; | Country == \u0026quot;United Arab Emirates\u0026quot; | Country == \u0026quot;Kuwait\u0026quot; |\rCountry == \u0026quot;Switzerland\u0026quot; | Country ==\u0026quot;Norway\u0026quot;| Country == \u0026quot;United States\u0026quot;, \u0026quot;navy\u0026quot;,\u0026quot;blue\u0026quot;)) %\u0026gt;%\rmutate(Col_Country = as.factor(Col_Country))\r\rCreate Plot\rp \u0026lt;- ggplot(data, aes(x=GDP,y=SPI)) p\rp2 \u0026lt;- p + theme_hc() +\rgeom_point(colour=\u0026quot;black\u0026quot;, size = 2.7, pch=21, aes(fill=Col_Country)) +\rscale_fill_manual(values = c(\u0026quot;#86d7f2\u0026quot;,\u0026quot;#00485d\u0026quot;), labels = NULL)\rp2\rp3 \u0026lt;- p2 + geom_smooth(method = \u0026quot;loess\u0026quot;,\rspan = 1.7,\rformula = y~x,\rse = FALSE,\rcolor = \u0026quot;#bf1912\u0026quot;, size = 1.5)\rp3\rp4 \u0026lt;- p3 + labs(title=\u0026quot;Measuring development\u0026quot;,\rsubtitle=\u0026quot;Social progress index and GDP per person\u0026quot;,\ry=\u0026quot;Social progress index, 2014\u0026quot;,\rx=\u0026quot;GDP per person, 2012, $ at PPP*\u0026quot;,\rcaption=\u0026quot;*Purchasing-Power Parity, 2005 prices\u0026quot;) +\rscale_y_continuous(breaks =seq(20,90,10), limits = c(20,90)) +\rscale_x_continuous(breaks = seq(0,50000,10000) , limits = c(0,50000), labels = comma) +\rtheme(plot.title = element_text(size = 11, face = \u0026quot;bold\u0026quot;, hjust = 0.0, color = \u0026quot;black\u0026quot;),\rplot.subtitle = element_text(size = 9, hjust = 0.0, color = \u0026quot;black\u0026quot;),\rplot.caption = element_text(size = 8, color = \u0026quot;black\u0026quot;),\rlegend.position = \u0026quot;none\u0026quot;,\raxis.title = element_text(size = 8, face = \u0026quot;italic\u0026quot;))\rp4\rlabelpoin \u0026lt;- c(\u0026quot;Costa Rica\u0026quot;,\u0026quot;Brazil\u0026quot;,\u0026quot;China\u0026quot;,\u0026quot;Chad\u0026quot;,\u0026quot;Angola\u0026quot;,\r\u0026quot;India\u0026quot;,\u0026quot;Iran\u0026quot;,\u0026quot;Philippines\u0026quot;,\u0026quot;Jamaica\u0026quot;,\u0026quot;Russia\u0026quot;, \u0026quot;Greece\u0026quot;,\u0026quot;Italy\u0026quot;,\u0026quot;Israel\u0026quot;,\u0026quot;Saudi Arabia\u0026quot;,\u0026quot;France\u0026quot;,\r\u0026quot;Korea\u0026quot;,\u0026quot;Japan\u0026quot;,\u0026quot;Britain\u0026quot;,\u0026quot;Germany\u0026quot;,\u0026quot;Canada\u0026quot;,\u0026quot;United Arab Emirates\u0026quot;,\r\u0026quot;Kuwait\u0026quot;,\u0026quot;Switzerland\u0026quot;,\u0026quot;Norway\u0026quot;,\u0026quot;United States\u0026quot;)\rset.seed(2012)\rp5 \u0026lt;- p4 + geom_text_repel(aes(label = Country),\rcolor = \u0026quot;black\u0026quot;, data = subset(data, Country %in% labelpoin),\rforce = 30)\rp5\r# add multiple caption\rp6 \u0026lt;- ggplotGrob(p5)\rk \u0026lt;- which(p6$layout$name==\u0026quot;caption\u0026quot;)\rgrbTxt \u0026lt;- p6$grobs[[k]]$children[[1]]\rgrbTxt$label \u0026lt;- \u0026quot;Source: Social Progress Imperative\u0026quot;\rgrbTxt$name \u0026lt;- \u0026quot;GRID.text.left\u0026quot;\rgrbTxt$x \u0026lt;- unit(0,\u0026quot;npc\u0026quot;)\rgrbTxt$hjust \u0026lt;- 0\rgrbTxt$gp$col \u0026lt;- \u0026quot;black\u0026quot;\rp6$grobs[[k]] \u0026lt;- addGrob(p6$grobs[[k]],grbTxt)\rgrid.draw(p6)\r\r","permalink":"http://ahmadhusain.in/post/2018-09-15-reproduce-economist-plot/","summary":"Here is a tutorial how to replicate an economist publication plot using ggplot2. Please prepare the library required first and download the data on the link below to follow the code.\nlink for download the report: https://www2.deloitte.com/content/dam/Deloitte/cr/Documents/public-sector/2014-Social-Progress-IndexRepIMP.pdf\nLoad Library\rlibrary(tidyverse)\rlibrary(pdftools)\rlibrary(ggthemes)\rlibrary(ggpubr)\rlibrary(tm)\rlibrary(grid)\rlibrary(ggrepel)\rlibrary(scales)\r\rImport Data\rdatatext \u0026lt;- pdf_text(\u0026quot;data_input/2014-Social-Progress-IndexRepIMP.pdf\u0026quot;)\r\rData Pre-Processing\rdatatext88 \u0026lt;- datatext[[88]] %\u0026gt;%\rread_lines()\rdata1 \u0026lt;- datatext88[7:50]\rhead(data1)\r#\u0026gt; [1] \u0026quot; 1 New Zealand $25,857 88.","title":"Reproduce Economist Plot"},{"content":"\r\rBackground\rFood security is one of the main priorities in the National Development Plan. The availability of strategic food is highly relied as an effort to achieve food security. Strategic food can be defined as a food commodity that is related to the needs of most people. One example of a strategic food commodity according to the Indonesian Ministry of Agriculture is chili/pepper. Most of the time the farmers experience crop failure due to bacterial and fungal disease attacks on the roots and leaves. One of the dominant diseases that attack pepper plants is the fungus Phytophthora capsici.\nIt is necessary to conduct a study to determine the location of the genes that affect the susceptibility of Phytophthora capsici. Several research results have proven that the Quantitative Trait Locus analysis is successful in identifying traits in plants. Therefore, as a study material for improving the quality of pepper plants, using the available DNA sequence data, using the QTL method approach, we will identify the location of the genes that are significantly associated with Phytophthora capsici disease.\n\rQuantitative Trait Loci\rIn a nutshell, the QTL is an individual chromosome locus where there are genes responsible for variations in a trait such as large fruit, sweet taste, level of spiciness, curly hair etc. Most of the quantitative traits in individuals are controlled by a number of genes.\nTo find out complex genetic trait, firstly we need to utilize the wide genetic diversity of the observed individuals by looking at the genetic linkage map. High density genetic linkage maps (centiMorgan) serve as marker-based selection to speed up breeding programs and to detect characterization of loci controlling for individual quantitative traits.\n\rModelling\rlibrary(ASMap)\rlibrary(qtlcharts)\rlibrary(qtl)\rlibrary(ggplot2)\rlibrary(ggpubr)\rlibrary(ggdendro)\rlibrary(factoextra)\rlibrary(car)\rlibrary(car)\rlibrary(dplyr)\rdata \u0026lt;- read.cross(\rformat = \u0026quot;csv\u0026quot;, file =\u0026quot;http://bit.ly/dataqtl\u0026quot;, sep = \u0026quot;;\u0026quot;, genotypes = c(\u0026quot;a\u0026quot;,\u0026quot;h\u0026quot;,\u0026quot;b\u0026quot;),\ralleles = c(\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;)\r)\r#\u0026gt; --Read the following data:\r#\u0026gt; 296 individuals\r#\u0026gt; 26 markers\r#\u0026gt; 1 phenotypes\r#\u0026gt; --Cross type: f2\rread.cross() is a function to read QTL data. genotypes defines a vector or string of genetic code. Based on data, code “a” means homozygous dominant (resistant to the viruses), code “h” for homozygous recessive (susceptible to the viruses).\nsummary(data)\r#\u0026gt; F2 intercross\r#\u0026gt; #\u0026gt; No. individuals: 296 #\u0026gt; #\u0026gt; No. phenotypes: 1 #\u0026gt; Percent phenotyped: 100 #\u0026gt; #\u0026gt; No. chromosomes: 1 #\u0026gt; Autosomes: 5 #\u0026gt; #\u0026gt; Total markers: 26 #\u0026gt; No. markers: 26 #\u0026gt; Percent genotyped: 100 #\u0026gt; Genotypes (%): aa:24.0 ab:48.4 bb:27.6 not bb:0.0 not aa:0.0\rThe phenotype data describes the scoring of the resistance of pepper plants to the virus. The scores made are in the range of 0 to 5. Pepper plants that are resistant to virus will be given a score of 0. While those that are very susceptible will be given a score of 5.\nQTL Mapping\rInterval Mapping is a popular approach to QTL analysis. Each sequence marker will be calculated with Logarithm of the Odds (LOD) value. LOD score is a statistical value used in genetic data to measure whether the 2 or more genes being observed tend to be located close to each other or not. A LOD score of 3 or more generally means that the 2 genes are located close together on the chromosome.\nThe simplest method in QTL mapping analysis is Marker Regression or commonly known as Analysis of Variance (ANAVA). But there are three drawbacks Marker Regression method for QTL mapping. First, it cannot differentiate between the predicted location of QTL and the influence of QTL. The location of the QTL is estimated only by looking at the markers with the largest LOD score, not by testing the hypothesis. The second, it cannot be recommended when there is missing in the markers data. And the last, when the markers are far apart, it will imply weakening of the evidence to detect QTL.\nInterval mapping is used as a solution to the three problems found in the analysis of variance or Marker Regression. Interval mapping is currently the most popular approach to QTL mapping. In interval mapping, each locus marker is considered one by one and then the LOD score is calculated to determine the actual QTL locus model made.\n# Marker regression method\rdata_mr \u0026lt;- data %\u0026gt;% est.rf(maxit = 200, tol = 1e-8) %\u0026gt;% scanone(method = \u0026quot;mr\u0026quot;)\r# Harley-Knott Regression\rdata_hk \u0026lt;- data %\u0026gt;% est.rf(maxit = 200, tol = 1e-8) %\u0026gt;% calc.genoprob(step = 1, error.prob = 0.001, map.function = \u0026quot;haldane\u0026quot;) %\u0026gt;% scanone(method = \u0026quot;hk\u0026quot;)\r# Multiple Imputation\rset.seed(120)\rdata_imp \u0026lt;- data %\u0026gt;% est.rf(maxit = 200, tol = 1e-8) %\u0026gt;% sim.geno(step = 1, error.prob = 0.001) %\u0026gt;% scanone(method = \u0026quot;imp\u0026quot;)\rdata_mr %\u0026gt;% arrange(desc(lod)) %\u0026gt;% head()\r#\u0026gt; chr pos lod\r#\u0026gt; MCA32 5 256.479 8.591757\r#\u0026gt; MCA12 5 129.609 8.115671\r#\u0026gt; PMMCB105 5 139.883 7.640826\r#\u0026gt; MCA9 5 149.926 7.340591\r#\u0026gt; PMMCB81 5 117.338 5.832687\r#\u0026gt; PMMCB32 5 145.402 5.791428\rWhen executed, it will produce a data object with 2 classes, namely scanone and dataframe. The data frame consists of four columns of information name of marker, location of the chromosomes, position of the genetic map (cM), and LOD score using each method. For instance, we print the data_mr and Marka MCA32 obtained the highest LOD score compared to the others around to 8,592.\nplot(\rdata_imp,\rdata_hk,\rdata_mr,\rylab = \u0026quot;LOD Score\u0026quot;,\rlty = c(1, 1, 2),\rcol = c(\u0026quot;black\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;),\rlwd = 2.5,\rylim = c(0, 10)\r)\rlegend(\r\u0026quot;topleft\u0026quot;,\rlegend = c(\u0026quot;Multiple Imputation\u0026quot;,\r\u0026quot;Harley-Knott Regression\u0026quot;, \u0026quot;Extented HK\u0026quot;),\rcol = c(\u0026quot;black\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;),\rlty = c(1, 1, 2),\rcex = 0.7,\rlwd = 2,\rtitle = \u0026quot;Metode\u0026quot;\r)\r\rModel Selection\rIn model selection, there are two approaches, forward selection and backward elimination. Forward selection is the stage of selecting the model by ‘forward’ selecting the most significant markers one by one. This process will stop when there are more markers that are potentially significant to the phenotype. While backward elimination, is the stage of selecting the model by ‘backward’ removing the markers that have the least significant effect.\ndata_sim \u0026lt;- sim.geno(\rcross = data,\rstep = 0,\rerror.prob = 0.001,\rmap.function = \u0026quot;haldane\u0026quot;, n.draws = 296\r)\rset.seed(1)\routsw \u0026lt;- stepwiseqtl(data_sim, verbose = TRUE, method = \u0026quot;imp\u0026quot;)\routsw\rSelection of the best model is determined by the highest penalized LOD value.\nsummary(outsw)\r#\u0026gt; QTL object containing imputed genotypes, with 296 imputations. #\u0026gt; #\u0026gt; name chr pos n.gen\r#\u0026gt; Q1 5@117.3 5 117.34 3\r#\u0026gt; Q2 5@159.3 5 159.31 3\r#\u0026gt; Q3 5@256.5 5 256.48 3\r#\u0026gt; #\u0026gt; Formula: y ~ Q1 + Q2 + Q3 + Q1:Q2 #\u0026gt; #\u0026gt; pLOD: 11.326\rFormula Model:\n\\[y \\sim Q1 + Q2 + Q3 + Q1:Q2 \\]\n\ry: Resistance phenotype score\r\\(Q_i\\): The i-th QTL marker\r\rafter obtaining significant variables, we will create a QTL object and re-model (retrain) it with fitqtl.\nchr \u0026lt;- c(5, 5, 5)\rpos \u0026lt;- c(117.34, 159.31, 256.48)\rqtl \u0026lt;- makeqtl(data_sim, chr, pos)\rmy.formula \u0026lt;- y ~ Q1 + Q2 + Q3 + Q1:Q2\rout.fitqtl \u0026lt;- fitqtl(data_sim,\rqtl = qtl,\rformula = my.formula,\rget.ests = F)\rsummary(out.fitqtl)\r#\u0026gt; #\u0026gt; fitqtl summary\r#\u0026gt; #\u0026gt; Method: multiple imputation #\u0026gt; Model: normal phenotype\r#\u0026gt; Number of observations : 296 #\u0026gt; #\u0026gt; Full model result\r#\u0026gt; ---------------------------------- #\u0026gt; Model formula: y ~ Q1 + Q2 + Q3 + Q1:Q2 #\u0026gt; #\u0026gt; df SS MS LOD %var Pvalue(Chi2) Pvalue(F)\r#\u0026gt; Model 10 366.9595 36.695946 24.7971 32.00891 0 0\r#\u0026gt; Error 285 779.4696 2.734981 #\u0026gt; Total 295 1146.4291 #\u0026gt; #\u0026gt; #\u0026gt; Drop one QTL at a time ANOVA table: #\u0026gt; ---------------------------------- #\u0026gt; df Type III SS LOD %var F value Pvalue(Chi2) Pvalue(F) #\u0026gt; 5@117.3 6 206.85 15.13 18.043 12.605 0.000 1.30e-12 ***\r#\u0026gt; 5@159.3 6 162.83 12.19 14.203 9.923 0.000 6.01e-10 ***\r#\u0026gt; 5@256.5 2 31.92 2.58 2.784 5.836 0.003 0.00328 ** #\u0026gt; 5@117.3:5@159.3 4 151.49 11.42 13.214 13.847 0.000 2.47e-10 ***\r#\u0026gt; ---\r#\u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rWe will summarize the important information in the following table:\ntibble(\rVariabel = c(\u0026quot;Q1\u0026quot;, \u0026quot;Q2\u0026quot;, \u0026quot;Q3\u0026quot;, \u0026quot;Q1:Q2\u0026quot;),\r`Kode Marka` = c(\u0026quot;PMMCB81\u0026quot;, \u0026quot;PMMCB34\u0026quot;, \u0026quot;MCA32\u0026quot;, \u0026quot;PMMCB81 : PMMCB34\u0026quot;),\r`% Var` = c(18.25, 13.36, 2.79, 12.62)\r)\r#\u0026gt; # A tibble: 4 x 3\r#\u0026gt; Variabel `Kode Marka` `% Var`\r#\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r#\u0026gt; 1 Q1 PMMCB81 18.2 #\u0026gt; 2 Q2 PMMCB34 13.4 #\u0026gt; 3 Q3 MCA32 2.79\r#\u0026gt; 4 Q1:Q2 PMMCB81 : PMMCB34 12.6\rThe first and second columns describe the symbol of the QTL model and its name. The percentage variance column (% var) is an estimate of the phenotype variance described by the PMMCB81, PMMCB34, MCA32 markers, and the PMMCB81: PMMCB34 marker interactions. The total % var was 46.3%. This means that the ability of all markers in the model to explain the phenotype variance scores of resistance to phytophthora capsici bacteria is 46.3%, while the rest is explained by other markers outside the observation.\nThe visualization of the genetic map with a significant QTL model and the result of the multiple imputation method LOD score is presented in the figure below.\npar(mfrow=c(1,2))\rplot(\routsw,\rcol = \u0026quot;red\u0026quot;,\rjustdots = F,\rshow.marker.names = F\r)\rplot(\rdata_imp$lod,\rdata_imp$pos,\rcol = \u0026quot;red\u0026quot;,\rxlab = \u0026quot;LOD\u0026quot;,\rylim = c(315, 0),\rlas = 1,\rylab = \u0026quot;Map Position (cM)\u0026quot;,\rtype = \u0026quot;l\u0026quot;,\rlwd = 3,\rmain = \u0026quot;Interval Mapping\u0026quot;\r)\rlegend(\r\u0026quot;bottomright\u0026quot;,\rlegend = \u0026quot;Multiple Imputation\u0026quot;,\rcol = \u0026quot;red\u0026quot;,\rlty = 1,\rcex = 0.7,\rlwd = 2,\rtitle = \u0026quot;Metode\u0026quot;,\rbty = \u0026quot;n\u0026quot;\r)\r\r\r","permalink":"http://ahmadhusain.in/post/qtl-mapping/","summary":"Background\rFood security is one of the main priorities in the National Development Plan. The availability of strategic food is highly relied as an effort to achieve food security. Strategic food can be defined as a food commodity that is related to the needs of most people. One example of a strategic food commodity according to the Indonesian Ministry of Agriculture is chili/pepper. Most of the time the farmers experience crop failure due to bacterial and fungal disease attacks on the roots and leaves.","title":"Quantitative Trait Loci (QTL) Mapping"},{"content":" one of 5 holders of RStudio Trainer Certification in Indonesia.\n Experiences  Sr. Instructor at Algoritma Data Science Education Center  Sep, 2018 - Present   Laboratory Assistant at Islamic University of Indonesia  Exploratory Data Analysis Mar, 2018 - Aug, 2018   Lecture Assistant at Islamic University of Indonesia  Mar, 2018 - Aug, 2018   Analyst - Internship Program at PT. Astra International Tbk - AHM  Nov, 2017 - Dec, 2017   Biotech - Internship program at PT. East West Seed Indonesia  Jan, 2017 - Feb, 2017    Education  B.S. in Subject: Statistics at Islamic University of Indonesia  2014 - 2018    DS Mentor Project 🤓   Course Production Algoritma Data Science Academy\n  In-house corporate training course production machine learning specialization at PT. Indo Tambangraya Megah.\n  In-house corporate training course production machine learning specialization at PT. Sigma Cipta Caraka.\n  In-house corporate training course production data visualization specialization at PT. OCBC NISP.\n  Twitter Sentiment Analysis Mini Data Science Series in collaboration with Tokopedia.\n  Algoritma Data Science Series Automate Business Reporting with R.\n  In-house Corporate Training course production Business Translator at PT. Indo Tambangraya Megah.\n  In-house Corporate Training course production Data Visualization at SKK Migas.\n  In-house Corporate Training course production Machine Learning specialization at Permata Bank.\n    Algoritma Academy Project Mentor:\n  Build a Recommendation System to show what movies to watch next, by using Memory-Based Collaborative Filtering Machine Learning Algorithm.\n  RFM Analysis Dashboard: Getting knowledge how the customers are behaving from purchase recency, purchase frequency, and monetary value data.\n    Indosat Capstone Project Mentor:\n  Predicting B2B Customer Repurchase using Naive Bayes Classifier and B2B Recommendation System Dashboard.\n  Predictive Modelling of FAB Opportunity Status using Logistic Regression.\n  Forecasting Warehouses using Time-Series Predictive Model.\n  International Carriers Clustering and Traffic Anomaly Detection using R.\n  Forecasting Data Revenue Indosat Ooredoo using Multi Seasonal Time Series.\n    BCA Capstone Project Mentor:\n  Predictive system for repayment probability of the Housing Ownership Credit debtor.\n  Implementation text mining for the customer\u0026rsquo;s complaint identification system.\n  Machine learning application for the recommendation and prediction for audit findings.\n  Implementing text mining for negative account information record system.\n  Time to defult in credit scoring using survival analysis\n  Identifying cross selling opportunities on API product using Collaborative filtering.\n    ","permalink":"http://ahmadhusain.in/about/","summary":"one of 5 holders of RStudio Trainer Certification in Indonesia.\n Experiences  Sr. Instructor at Algoritma Data Science Education Center  Sep, 2018 - Present   Laboratory Assistant at Islamic University of Indonesia  Exploratory Data Analysis Mar, 2018 - Aug, 2018   Lecture Assistant at Islamic University of Indonesia  Mar, 2018 - Aug, 2018   Analyst - Internship Program at PT. Astra International Tbk - AHM  Nov, 2017 - Dec, 2017   Biotech - Internship program at PT.","title":"About Me"},{"content":"C19-Dashboard  COVID-19 Dashboard Monitoring https://ahmadhusain.shinyapps.io/covid19-dashboard/\n Dashboard monitor pandemic situation globally and in Indonesia. Built using Shiny and Shinydashboard in R. This project aims to provide the information visually the total number of confirmed cases, recovered, and death. The data will be updated automatically every hour.\nImage-Classification with Random Forest an example script if you want to do image-classification stuff with Random Forest. The datasets are available at Malaria Dataset cell_images.zip. Please download so you can follow the script.\nHere is a quick brief of the project workflow.\nIf you want to try predict an image, please use one of the cell images listed here.\n Ya, we know it\u0026rsquo;s not a best measure to inference image by using Random Forest :p\n R package: cmplot  cmplot is a developed packages for create confusion matrix graph for Optimizing Probability Thresholds.\n You can install the development version of cmplot using:\n# install.packages(\u0026quot;remotes\u0026quot;)\rremotes::install_github(\u0026quot;ahmadhusain/cmplot\u0026quot;)\rlibrary(dplyr)\rlibrary(caret)\rlibrary(tidyr)\rlibrary(ggplot2)\rlibrary(plotly)\rlibrary(cmplot)\rload(\u0026quot;data-raw/churn_prediction.RData\u0026quot;)\rSave the probability results from your model and actual data in the following format:\nhead(data_pkg)\r   probability churn_status     0.294 Yes   0.794 Yes   0.275 No   0.161 No   0.00769 No   0.555 No    confmat_plot(data_pkg$probability, data_pkg$churn_status, \u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;)\rLearn-R Data Visualization  Apply sortable \u0026amp; parsons to learnr tutorials https://ahmadhusain.shinyapps.io/learnr-dv/\n learnr-dv is an interactive tutorial and exercise built with learnr, parsons and sortable packages. The tutorial consist of content about understanding the context of visualize the data using ggplot2.\nML in R Cheatsheet  Machine Learning Modelling in R Cheatsheet. https://github.com/ahmadhusain/mlinR-cheatsheet\n Is a cheat sheet to help determine what machine learning algorithms appropriate to used. Also for further improvement will be adding case examples for each machine learning method using R, obviously.\nCapstone Project Leaderboard  Shinyapps for automated scoring https://github.com/teamalgoritma/capstoneml-leaderboard\n build leaderboard using shiny for machine learning capstone project in Algoritma Data Science Academy.\n","permalink":"http://ahmadhusain.in/project/","summary":"C19-Dashboard  COVID-19 Dashboard Monitoring https://ahmadhusain.shinyapps.io/covid19-dashboard/\n Dashboard monitor pandemic situation globally and in Indonesia. Built using Shiny and Shinydashboard in R. This project aims to provide the information visually the total number of confirmed cases, recovered, and death. The data will be updated automatically every hour.\nImage-Classification with Random Forest an example script if you want to do image-classification stuff with Random Forest. The datasets are available at Malaria Dataset cell_images.","title":"Projects"}]