<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Text Classification with LSTM | Husain&#39;s Blog</title>
<meta name="keywords" content="Deep Learning, Classification, Supervised Learning, NLP, LSTM" />
<meta name="description" content="Deep Neural NetworkBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified.">
<meta name="author" content="Ahmad Husain Abdullah">
<link rel="canonical" href="http://ahmadhusain.in/post/text-lstm/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.ade87ea3120e9a1c6cc946e04768c952fa699fdea531ca3a5b082b605b467f77.css" integrity="sha256-reh&#43;oxIOmhxsyUbgR2jJUvppn96lMco6WwgrYFtGf3c=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/img/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/img/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/img/favicon.png">
<link rel="apple-touch-icon" href="http://ahmadhusain.in/apple-touch-icon.png">
<link rel="mask-icon" href="http://ahmadhusain.in/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.82.0" />
<link rel="alternate" hreflang="en" href="http://ahmadhusain.in/post/text-lstm/" />
<meta property="og:title" content="Text Classification with LSTM" />
<meta property="og:description" content="Deep Neural NetworkBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ahmadhusain.in/post/text-lstm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2018-09-21T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2019-09-21T22:05:18&#43;07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Text Classification with LSTM"/>
<meta name="twitter:description" content="Deep Neural NetworkBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://ahmadhusain.in/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Text Classification with LSTM",
      "item": "http://ahmadhusain.in/post/text-lstm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Text Classification with LSTM",
  "name": "Text Classification with LSTM",
  "description": "Deep Neural Network\rBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified.",
  "keywords": [
    "Deep Learning", "Classification", "Supervised Learning", "NLP", "LSTM"
  ],
  "articleBody": "\rDeep Neural Network\rBefore we further discuss the Long Short-Term Memory Model, we will first discuss the term of Deep learning where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified. Simple Neural Network only has 1 hidden layer, whereas Deep Learning has more than 1 hidden layer.\nNerual Network architecture can also be characterized based on the direction of signal in training process: feed-forward and recurrent. In this material, we will discuss more the Recurrent Neural Network architecture.\nWe know that neural network uses an algorithm called Backpropagation to update the weights of the network. So what Backpropogation does is it first calculates the gradients from the error using the chain rule, then in updates the weights (Gradient Descent).\nWhen doing backpropogation in simple neural network (1 hidden layer) we might not encounter the problem update weights. But….\n\rWhen we build an architecture with a large number of hidden layer (Deep Neural Network) the model is likely to encounter update weight problem called vanishing / exploding gradient descent.\n\rVanishing / Exploding Gradient Descent\rVanishing Gradient Descent: the update value obtained will exponentially decrease when heading to the input layer. Here are the illustrations, which I took from: Michael Nguyen’s article\nGradient descent aims to adjust weights that allow the model to ‘learn’. the nature of the gradient that is, the greater the value of the gradient in the current layer, will affect in the next layer getting bigger. and vice versa. This is the problem. When doing BP, each node will calculate the gradient value and update its weight according to the gradient effect on the previous layer. so if the previous layer is small, then adjusting the weights in the current layer will be small. it causes the gradient to shrink exponentially when it goes to the input layer. so that when in the input layer it fails to do the learning due to vanishing gradient problems. so the model fails to learn when a pass forward is made again to make predictions.\nExploding Gradient Descent: the update value obtained will exponentially increase when heading to the input layer. The characteristics of the model have an exploding gradient problem, which is when the cost function results are NaN.\nHere is a link duscussion regarding to exploding grgadient descent: nan loss when training NN\n\r\rRecurrent Neural Network\rFrom the vasishing / exploding gradient problem mentioned above, the development of architecture from the RNN, namely LSTM and GRU, is able to handle the problem. will be discussed below. RNN itself has not been able to handle vanishing gradients due to short-term memory problems.\nSo what is RNN? RNN is a deep learning architecture model that is commonly used for sequential data. What’s the sequential data? The following are examples of sequential data cases:\nSentiment classification. Input: text, output: rating/sentiment class.\rTranslator. Input: text, output: text translator.\rTime series data: input: Numeric, output: forecasting result.\rSpeech Recognation: input: audio, output: text.\r\rRNN Concept\rThe main idea of RNN is to utilize sequential information processing. RNN is called repetitive because it performs the same task for each successive element, with output depending on the previous calculation.\nThe diagram above is the architecture of RNN after opening it unfolded. Suppose we have a sentence consisting of 10 words, it means that there will be 10 NN-layers formed separately. Each layer represents each word. The following are some explanations of the notation in the diagram above:\n\r\\(x_t\\) is an input at (time) \\(t\\).\rRNN stores an internal state \\(S_t\\) which becoming a memory in RNN. \\(S_t\\) is calculated based on the previous hidden state and the input in the current step. The activation function of \\(f\\) is usually nonlinear like tanh or ReLU.\r\r\r\\(S_t = f(U_{xt} + W_{St} − 1)\\).\r\r\n\r\\(O_t\\) is an output from each step \\(t\\)\r\rForward Pass\rFor example case studies related to sentiment classification. Then the input will contain one-hot vectors of words, and the output is a sentiment class. So a feed-forward pass scheme is performed as:\nEach layer describes each word.\rFor an explanation of the concept of the Forward pass, we look at the layer when \\(t\\) (in the middle).\r\r\rRemember that the RNN pays attention to the output calculation data at \\(t - 1\\). So initially we have to calculate the state at \\(S_t\\) first. Involving the multiplication of the input matrix \\(x_t\\) with the parameter \\(U\\) and summing the results of the product \\(s_{t1}\\). Then the results are processed with the tanh activation function. The calculation details are as follows:\r\r\r\\(s_t=tanh(U.x_t+W.s_{t−1)}\\)\r\r\n\rThe results from \\(s_t\\) are then passed to the output by tiling the matrix multiplication of the \\(V\\) parameters and then passed with the softmax activation function. The details are as follows:\r\r\r\\(\\hat{y}_t=softmax(V.s_t)\\)\r\r\nThe above process is often illustrated like:\n\rBackpropogation Through Time\rThe purpose of the RNN model training is to find the parameters \\(U, V,\\) and \\(W\\) which produce minimum errors. The term BTT arises because the RNN architecture pays attention to the previous time series. So to calculate the gradient at the time step \\(t\\), we must calculate the speech at step \\(t − 1, t − 2, t − 3\\) until it is at time \\(t = 1\\). If you are curious about how this algorithm works, I suggest to read more on this article: Part 3: Backpropogarion \u0026 Vanishing Gradient.\nAs mentioned in the previous chapter, the RNN model also has a vanishing gradient problem, because it cannot capture long-term dependencies. because the number of layers is too long, making the backprop process produces a gradient value that is getting smaller and even close to zero or is said to disappear when it arrives at the initial layers.\n\rThis is caused by the multiplication properties between fractions. Imagine that for example a fraction is 1/4, multiplied by another fraction such as 1/3, then in one operation the value is 1/12. Multiplied by other fractions such as 1/5, the value becomes 1/60, etc. This value will shrink exponentially, and with a small fractional value and many multiplication operations, the value will be close to zero.\n\rTo overcome this problem, there is a development of RNN model namely Long-Term Short Memory (LSTM).\n\r\r\rLSTM\rJust like RNN, LSTM has a sequential model which is illustrated with a green box. if unfolded the architecture becomes as below:\nThe difference between RNN and LSTM is that it has additional signal information that is given from one time step to the next time step which is commonly called “cell memory”. LSTM is designed to overcome the problem of vanishing gradient, using the gate mechanism.\nLSTM Network\rSo the components in LSTM consist of:\n\rForget Gate f (NN with sigmoid as activation function).\rCandidate Layer g (NN with tanh as activation function).\rInput Gate I (NN with sigmoid as activation function).\rOutput Gate O (NN with sigmoid as activation function).\rHidden State H (vector).\rMemory State C (vector).\r\rThe following is the LSTM diagram at the t-time step.\n\r\\(X_t\\) = Input vector at the t-time.\r\\(H_{t−1}\\) = Previous Hidden state.\r\\(C_{t−1}\\) = Previous Memory state.\r\\(H_t\\) = Current Hidden state.\r\\(C_t\\) = Current Memori state.\r[*] = multiplication operation.\r[+] = addition operation.\r\rso the input of each LSTM module is \\(X_t\\) (current input), \\(H_{t − 1}\\), and \\(C_{t − 1}\\). then the output is \\(H_t\\), and \\(C_t\\).\nextracted from Denny Britz’s article which is a summary of Christopher Olah’s article: Understanding LSTM Networks.\n\\(l, f, O\\) is the Input, Forget, and Output gates. Both input, forget, and output have the same function formula (sigmoid), which only distinguishes the matrix parameters (note the formula below). This means that the output of this gate has a vector of values between 0 to 1. zero means that the information is blocked completely, and one means that all information is included. The gate input controls how many states you have just computed for the current input that you want to let pass. The forget gate controls how many previous states you want to let pass. Finally, the gate output controls how many internal states you want to expose to the network (higher layer \u0026 next time step). All gates have the same dimensions as the hidden state dimension (etc.) as a measure for the hidden state. The output of the sigmoid gate will be multiplied by another value to control how much that value is used.\r\r\r\\(I= \\sigma(x_tU^I + s_{t-_1}W^I)\\) \\(f= \\sigma(x_tU^f + s_{t-_1}W^f)\\) \\(O= \\sigma(x_tU^O + s_{t-_1}W^O)\\) \r\rvalue 1 means “really take care of this element” while 0 means “completely get rid of this element”.\n\r\\(g\\) is a “candidate” hidden state that is computed based on the current input and the previous hidden state.\n\r\\(c_t\\) is the internal memory of the unit. It is a combination of the previous memory \\(c_{t-1}\\) multiplied by the forget gate, and the newly computed hidden state g, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input.\n\r\r\r\\(c_t=f_t*c_{t-_1} + I_t*g\\)\r\r\r\rImplementation LSTM with Keras\r# load packages required\rlibrary(keras)\rlibrary(RVerbalExpressions)\rlibrary(magrittr)\rlibrary(textclean)\rlibrary(tidyverse)\rlibrary(tidytext)\rlibrary(rsample)\rlibrary(yardstick)\rlibrary(caret)\r#set seed keras for reproducible result\ruse_session_with_seed(2)\r# set conda env\ruse_condaenv(\"tensorflow\")\rImport Data\rIn this example we will use a case study of sentiment tweets about airlines in the US obtained from Kaggle.\ndata \r## Observations: 14,640\r## Variables: 15\r## $ tweet_id  5.703061e+17, 5.703011e+17, 5.703...\r## $ airline_sentiment  \"neutral\", \"positive\", \"neutral\",...\r## $ airline_sentiment_confidence  1.0000, 0.3486, 0.6837, 1.0000, 1...\r## $ negativereason  NA, NA, NA, \"Bad Flight\", \"Can't ...\r## $ negativereason_confidence  NA, 0.0000, NA, 0.7033, 1.0000, 0...\r## $ airline  \"Virgin America\", \"Virgin America...\r## $ airline_sentiment_gold  NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ name  \"cairdin\", \"jnardino\", \"yvonnalyn...\r## $ negativereason_gold  NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ retweet_count  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r## $ text  \"@VirginAmerica What @dhepburn sa...\r## $ tweet_coord  NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ tweet_created  \"2015-02-24 11:35:52 -0800\", \"201...\r## $ tweet_location  NA, NA, \"Lets Play\", NA, NA, NA, ...\r## $ user_timezone  \"Eastern Time (US \u0026 Canada)\", \"Pa...\r\rText Pre-Processing\rSetup regex\rmention\rmention % rx_find(value = \"@\") %% rx_alnum() %% rx_one_or_more()\rmention\r## [1] \"(@)[A-z0-9]+\"\r\"@VirginAmerica What @dhepburn said.\" %% str_remove_all(pattern = mention) %% str_squish()\r## [1] \"What said.\"\r\rhashtag\rhashtag % rx_find(value = \"#\") %% rx_alnum() %% rx_one_or_more()\rhashtag\r## [1] \"(#)[A-z0-9]+\"\r\"@VirginAmerica I'm #elevategold for a good reason: you rock!!\" %% str_remove_all(pattern = mention) %%\rstr_remove_all(pattern = hashtag) %% str_squish()\r## [1] \"I'm for a good reason: you rock!!\"\r\rquestion mark\rquestion % rx_find(value = \"?\") %% rx_one_or_more()\rquestion\r## [1] \"(\\\\?)+\"\r\rexclamation mark\rexclamation % rx_find(value = \"!\") %% rx_one_or_more()\rexclamation\r## [1] \"(!)+\"\r\rpunctuation\rpunctuation \r## [1] \"[[:punct:]]\"\r\rnumber\rnumber \r## [1] \"\\\\d\"\r\rdollar sign\rdollar % rx_find(\"$\")\rdollar\r## [1] \"(\\\\$)\"\r\r\rText Cleansing\rreplace_url\r\"@VirginAmerica Really missed a prime opportunity, there. https://t.co/mWpG7grEZP\" %% replace_url()\r## [1] \"@VirginAmerica Really missed a prime opportunity, there. \"\r\rreplace_emoticon\r\"@SouthwestAir thanks! Very excited to see it :3\" %%\rreplace_emoticon()\r## [1] \"@SouthwestAir thanks! Very excited to see it smiley \"\r\rreplace_contruction\r\"@united I'd thank you - but you didn't help. taking 6 hours to reply isn't actually helpful\" %% replace_contraction()\r## [1] \"@united I would thank you - but you did not help. taking 6 hours to reply is not actually helpful\"\r\rreplace_word_elongation\r\"@VirginAmerica heyyyy guyyyys.. :/\" %% replace_word_elongation()\r## [1] \"@VirginAmerica hey guys.. :/\"\rdata % mutate(\rtext_clean = text %% replace_url() %% replace_emoji() %% replace_emoticon() %% replace_html() %% str_remove_all(pattern = mention) %% str_remove_all(pattern = hashtag) %% replace_contraction() %% replace_word_elongation() %% str_replace_all(pattern = question, replacement = \"questionmark\") %% str_replace_all(pattern = exclamation, replacement = \"exclamationmark\") %% str_remove_all(pattern = punctuation) %% str_remove_all(pattern = number) %% str_remove_all(pattern = dollar) %% str_to_lower() %% str_squish()\r)\rdata %% select(text, text_clean) %% sample_n(20)\r## # A tibble: 20 x 2\r## text text_clean ##   ## 1 @united How can I file a claim whe~ how can i file a claim when your ag~\r## 2 @USAirways Seriously. You can't tw~ seriously you can not tweet and let~\r## 3 @JetBlue thank you will do!! You g~ thank you will doexclamationmark yo~\r## 4 @united what's the hold up with fl~ what is the hold up with flight fro~\r## 5 @AmericanAir ticks me off. ticks me off ## 6 @AmericanAir but at least your aut~ but at least your automated message~\r## 7 @united the hotel you sent us to w~ the hotel you sent us to wouldnt ta~\r## 8 @AmericanAir your definition and m~ your definition and mine of min is ~\r## 9 @JetBlue why are you always so ama~ why are you always so amazingexclam~\r## 10 @JetBlue and The from @WSJ Team to~ and the from team to offer in acces~\r## 11 @JetBlue Oh that totally looks on ~ oh that totally looks on par with s~\r## 12 \"@united Thank you!! \\U0001f60a\" thank youexclamationmark smiling fa~\r## 13 @USAirways Still irritated that ou~ still irritated that our well plann~\r## 14 @united Sure. Follow for a sec an~ sure follow for a sec and i will ## 15 \"@SouthwestAir your A-list program~ your alist program is a complete jo~\r## 16 @AmericanAir Thanks, both airlines~ thanks both airlines said that it i~\r## 17 @AmericanAir Thank you. Good sugge~ thank you good suggestion i checked~\r## 18 @united care less about the person~ care less about the person although~\r## 19 @SouthwestAir I was helped by a ni~ i was helped by a nice lady at phx ~\r## 20 @united we've been here since 3am ~ we have been here since am and you ~\r\r\rprepare datainput\rdata % mutate(label = factor(airline_sentiment, levels = c(\"negative\", \"neutral\", \"positive\")),\rlabel = as.numeric(label),\rlabel = label - 1) %% select(text_clean, label) %% na.omit()\rhead(data, 10)\r## # A tibble: 10 x 2\r## text_clean label\r##  \r## 1 what said 1\r## 2 plus you have added commercials to the e tongue sticking out erie~ 2\r## 3 i did not today must mean i need to take another tripexclamationm~ 1\r## 4 it is really aggressive to blast obnoxious entertainment in your ~ 0\r## 5 and it is a really big bad thing about it 0\r## 6 seriously would pay a flight for seats that did not have this pla~ 0\r## 7 yes nearly every time i fly vx this ear worm won t go away smiley 2\r## 8 really missed a prime opportunity for men without hats parody the~ 1\r## 9 well i did not but now i doexclamationmark laughing 2\r## 10 it was amazing and arrived an hour early you are too good to me 2\r\r\rTokenizer\rTokenizer aims to separate each word in the entire document into a token form. The num_words parameter is for setting the maximum number of words to be used, sorted according to the largest frequency order. words that rarely appear will be removed. from a total of 13291 unique words contained in the text data, we reduced it to 1024 which will be used to make the model. The lower parameter is a logic condition, if TRUE then all words will be transformed to lowercase (tolower).\nnum_words % fit_text_tokenizer(data$text_clean)\rpaste(\"number of unique words:\", length(tokenizer$word_counts))\r## [1] \"number of unique words: 13291\"\rIntuition\rSuppose we have 5 pieces of text documents that are stored in the docs object. Then we made a token with a maximum of words / terms used, which is 4. It means that words that rarely appear will not be used during the train model. To see the number of unique words stored in the document dictionary, use the command token$word_counts. To see the list of words with the highest frequency of appearances, use the token$word_index command.\ndocs % fit_text_tokenizer(docs)\rpaste(\"number of unique words\",length(tokendocs$word_counts))\r## [1] \"number of unique words 8\"\rtokendocs$word_index[1:4]\r## $work\r## [1] 1\r## ## $well\r## [1] 2\r## ## $done\r## [1] 3\r## ## $good\r## [1] 4\r\r\rSplitting Data\rSplitting data will be done into 3 parts, namely train, validation, and test. The proportion is 60% for trains and the remaining 40% is in partitions for data validation and testing.\nData Train is the data that we will use to train the model. Data Validation for evaluating hyperparameter tuning in models (adjust hidden layers, optimizers, learning rates, etc.). While the test data as an evaluator of the model that we make on unseen data.\nset.seed(100)\rintrain \rmaxlen \r## [1] \"maxiumum length words in data: 90\"\r# prepare x\rdata_train_x %\rpad_sequences(maxlen = maxlen)\rdata_val_x %\rpad_sequences(maxlen = maxlen)\rdata_test_x %\rpad_sequences(maxlen = maxlen)\r# prepare y\rdata_train_y \rIntuition\rCommand texts_to_sequence() aims to create a matrix results of the transformation text to the form of a number sequence (integer). Then wrapped with the command pad_sequences() which aims to equalize the dimensions of the length on the entire document. Imagine the input layer of a matrix, it must have the same row and column. Therefore it is necessary to do padding. By default the value parameter will be set to 0. This means that if there are words that are not in our token (which has been limited by num_words) then it will be transformed to 0. The following is the illustration:\ntexts_to_sequences(tokendocs, c(\"Excellent!\", \"Good job bro, keep hard work\", \"well done\")) %% pad_sequences(maxlen = 5)\r## [,1] [,2] [,3] [,4] [,5]\r## [1,] 0 0 0 0 0\r## [2,] 0 0 0 0 1\r## [3,] 0 0 0 2 3\rThe result of text_to_sequences is a matrix of size \\(n ∗ maxlen\\). The example above consists of 3 text documents and is set to maxlen = 5. it will produce a \\(3 \\times 5\\) matrix which each index is a representative integer of the same words as tokendocs in the i-th list. Recall, the word done is on the 3rd list on our token, right? therefore the matrix result above in the third document and the last integer sequence is 3. Why does it appear in the last index? because in pad_sequences we don’t set parameters of the padding type whether “pre” or “post” and by default is “pre”.\ntokendocs$word_index[3]\r## $done\r## [1] 3\r\r\rArchitecture\rEmbedding Layer\rEmbedding Layers can only be used in the initial / first layer of the LSTM architecture. In a variety of deep learning frameworks such as Keras, the embedding layer aims to train text data into numerical vectors which represent the closeness of the meaning of each word.\nEmbedding layer accepts several parameters. Some examples are:\r* input_dim, which is the maximum dimension of the vocabulary that has been explained in the num_words section.\n\rinput_length, the maximum length of the word sequence in the document input.\n\routput_dim which is the embedding dimension of the output layer which will be passed to the next layer. generally is 32, but can be more dependent on the problem we face.\n\r\r\rInput received of 2D vectors with the form: {batch_size, sequence_length}, while the output received 3D tensor with the forms {batch_size, sequence_length, output_dim}.\n\r\rDeep Neural Layer\rThe Deep Network Layer accepts the embedding matrix as input and then is converted into smaller dimensions. The dimensions of the compression results have represented information from the data. In the case of data text, the deep learning architecture commonly used is RNN  LSTM / GRU.\n\ryou can check the Keras Documentation for the details sequential layers.\n\r\rOutput Layer\rThis output layer is the last layer in the deep learning architecture. At Keras use the layer_dense command where we need to set the unit parameters or how many neurons we want to build. In this case I use 3 units, because there are 3 classes we have (negative, neutral, positive).\n\rRandom Initialization\rWhen the neural network / deep learning model train often results in different results. Why? because NN and DL use weigth which is generated randomly (randomness initialization). therefore we need to set the numbers (x-random models) in order to get a fixed result when repeated in the train (reproducible result). this can be done with the seed parameter in the initializer_random_uniform command. for more details, read the question and answer article in Keras studio\n# initiate keras model sequence\rmodel %\r# layer input\rlayer_embedding(\rname = \"input\",\rinput_dim = num_words,\rinput_length = maxlen,\routput_dim = 32, embeddings_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r) %%\r# layer dropout\rlayer_dropout(\rname = \"embedding_dropout\",\rrate = 0.5\r) %%\r# layer lstm 1\rlayer_lstm(\rname = \"lstm\",\runits = 256,\rdropout = 0.2,\rrecurrent_dropout = 0.2,\rreturn_sequences = FALSE, recurrent_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2),\rkernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r) %%\r# layer output\rlayer_dense(\rname = \"output\",\runits = 3,\ractivation = \"softmax\", kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)\r)\r\rDropout parameters are added to reduce the risk of overfit. the range of dropout values between 0 to 1. commonly used is 0.2 to 0.5. the closer to 0 will tend to overfit, while the closer to 1 has the risk of underfit.\r\r\rCompiling Model\rFor two category classes, the lost function used is binary_crossentropy while for multiclass cases it uses categorical_crossentropy. There are not only 2 option, but the most common when working with classification cases, these 2 loss functions are used. Here are some loss function options from Keras Documentation\n# compile the model\rmodel %% compile(\roptimizer = \"adam\",\rmetrics = \"accuracy\",\rloss = \"categorical_crossentropy\"\r)\r# model summary\rsummary(model)\r## ___________________________________________________________________________\r## Layer (type) Output Shape Param # ## ===========================================================================\r## input (Embedding) (None, 90, 32) 32768 ## ___________________________________________________________________________\r## embedding_dropout (Dropout) (None, 90, 32) 0 ## ___________________________________________________________________________\r## lstm (LSTM) (None, 256) 295936 ## ___________________________________________________________________________\r## output (Dense) (None, 3) 771 ## ===========================================================================\r## Total params: 329,475\r## Trainable params: 329,475\r## Non-trainable params: 0\r## ___________________________________________________________________________\r\r\rTrain the Model\r# model fit settings\repochs % fit(\rdata_train_x, data_train_y,\rbatch_size = batch_size, epochs = epochs,\rverbose = 1,\rvalidation_data = list(\rdata_val_x, data_val_y\r)\r)\r# history plot\rplot(history)\r\rModel Evaluation\r# predict on train\rdata_train_pred %\rpredict_classes(data_train_x) %%\ras.vector()\r# predict on val\rdata_val_pred %\rpredict_classes(data_val_x) %%\ras.vector()\r# predict on test\rdata_test_pred %\rpredict_classes(data_test_x) %%\ras.vector()\r# accuracy on data train\raccuracy_vec(\rtruth = factor(data_train$label,labels = c(\"negative\", \"neutral\", \"positive\")),\restimate = factor(data_train_pred, labels = c(\"negative\", \"neutral\", \"positive\"))\r)\r## [1] 0.8056534\r# accuracy on data test\raccuracy_vec(\rtruth = factor(data_test$label,labels = c(\"negative\", \"neutral\", \"positive\")),\restimate = factor(data_test_pred, labels = c(\"negative\", \"neutral\", \"positive\"))\r)\r## [1] 0.7895461\r\r\r",
  "wordCount" : "4005",
  "inLanguage": "en",
  "datePublished": "2018-09-21T00:00:00Z",
  "dateModified": "2019-09-21T22:05:18+07:00",
  "author":{
    "@type": "Person",
    "name": "Ahmad Husain Abdullah"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://ahmadhusain.in/post/text-lstm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Husain's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://ahmadhusain.in/img/r-icon.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://ahmadhusain.in/" accesskey="h" title="Husain&#39;s Blog (Alt + H)">Husain&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://ahmadhusain.in/id/" title="Indonesian"
                            aria-label="Indonesian">Indonesian</a>
                    </li>
                </ul>
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="http://ahmadhusain.in/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://ahmadhusain.in/project" title="Project">
                    <span>Project</span>
                </a>
            </li>
            <li>
                <a href="http://ahmadhusain.in/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://ahmadhusain.in/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://ahmadhusain.in/">Home</a>&nbsp;»&nbsp;<a href="http://ahmadhusain.in/post/">Posts</a></div>
    <h1 class="post-title">
      Text Classification with LSTM
    </h1>
    <div class="post-meta">September 21, 2018&nbsp;·&nbsp;19 min&nbsp;·&nbsp;Ahmad Husain Abdullah
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#" aria-label="Deep Neural Network">Deep Neural Network</a><ul>
                        
                <li>
                    <a href="#" aria-label="Vanishing / Exploding Gradient Descent">Vanishing / Exploding Gradient Descent</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Recurrent Neural Network">Recurrent Neural Network</a><ul>
                        
                <li>
                    <a href="#" aria-label="RNN Concept">RNN Concept</a><ul>
                        
                <li>
                    <a href="#" aria-label="Forward Pass">Forward Pass</a></li>
                <li>
                    <a href="#" aria-label="Backpropogation Through Time">Backpropogation Through Time</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#" aria-label="LSTM">LSTM</a><ul>
                        
                <li>
                    <a href="#" aria-label="LSTM Network">LSTM Network</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Implementation LSTM with Keras">Implementation LSTM with Keras</a><ul>
                        
                <li>
                    <a href="#" aria-label="Import Data">Import Data</a></li>
                <li>
                    <a href="#" aria-label="Text Pre-Processing">Text Pre-Processing</a><ul>
                        
                <li>
                    <a href="#" aria-label="Setup regex">Setup regex</a><ul>
                        
                <li>
                    <a href="#" aria-label="mention">mention</a></li>
                <li>
                    <a href="#" aria-label="hashtag">hashtag</a></li>
                <li>
                    <a href="#" aria-label="question mark">question mark</a></li>
                <li>
                    <a href="#" aria-label="exclamation mark">exclamation mark</a></li>
                <li>
                    <a href="#" aria-label="punctuation">punctuation</a></li>
                <li>
                    <a href="#" aria-label="number">number</a></li>
                <li>
                    <a href="#" aria-label="dollar sign">dollar sign</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Text Cleansing">Text Cleansing</a><ul>
                        
                <li>
                    <a href="#" aria-label="replace_url"><code>replace_url</code></a></li>
                <li>
                    <a href="#" aria-label="replace_emoticon"><code>replace_emoticon</code></a></li>
                <li>
                    <a href="#" aria-label="replace_contruction"><code>replace_contruction</code></a></li>
                <li>
                    <a href="#" aria-label="replace_word_elongation"><code>replace_word_elongation</code></a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="prepare datainput">prepare datainput</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Tokenizer">Tokenizer</a><ul>
                        
                <li>
                    <a href="#" aria-label="Intuition">Intuition</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Splitting Data">Splitting Data</a><ul>
                        
                <li>
                    <a href="#" aria-label="Intuition">Intuition</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Architecture">Architecture</a><ul>
                        
                <li>
                    <a href="#" aria-label="Embedding Layer">Embedding Layer</a></li>
                <li>
                    <a href="#" aria-label="Deep Neural Layer">Deep Neural Layer</a></li>
                <li>
                    <a href="#" aria-label="Output Layer">Output Layer</a></li>
                <li>
                    <a href="#" aria-label="Random Initialization">Random Initialization</a></li>
                <li>
                    <a href="#" aria-label="Compiling Model">Compiling Model</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="Train the Model">Train the Model</a></li>
                <li>
                    <a href="#" aria-label="Model Evaluation">Model Evaluation</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">


<div id="deep-neural-network" class="section level1">
<h1>Deep Neural Network</h1>
<p>Before we further discuss the Long Short-Term Memory Model, we will first discuss the term of <em>Deep learning</em> where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified. Simple Neural Network only has 1 hidden layer, whereas Deep Learning has more than 1 hidden layer.</p>
<p><img src="img/lstm_text/dnn.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Nerual Network architecture can also be characterized based on the direction of signal in training process: <em>feed-forward</em> and <em>recurrent</em>. In this material, we will discuss more the <em>Recurrent Neural Network</em> architecture.</p>
<p>We know that neural network uses an algorithm called <strong>Backpropagation</strong> to update the weights of the network. So what Backpropogation does is it first calculates the gradients from the error using the chain rule, then in updates the weights (<em>Gradient Descent</em>).</p>
<p>When doing backpropogation in simple neural network (1 hidden layer) we might not encounter the problem update weights. But….</p>
<blockquote>
<p>When we build an architecture with a large number of hidden layer (Deep Neural Network) the model is likely to encounter update weight problem called <em>vanishing</em> / <em>exploding</em> gradient descent.</p>
</blockquote>
<div id="vanishing-exploding-gradient-descent" class="section level2">
<h2>Vanishing / Exploding Gradient Descent</h2>
<p><strong>Vanishing Gradient Descent</strong>: the update value obtained will exponentially decrease when heading to the input layer. Here are the illustrations, which I took from: <a href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9">Michael Nguyen’s article</a></p>
<p><img src="img/lstm_text/vg1.gif" width="35%" style="display: block; margin: auto;" /></p>
<p>Gradient descent aims to adjust weights that allow the model to ‘<strong>learn</strong>’. the nature of the gradient that is, the greater the value of the gradient in the current layer, will affect in the next layer getting bigger. and <em>vice versa</em>. This is the problem. When doing BP, each node will calculate the gradient value and update its weight according to the gradient effect on the previous layer. so if the previous layer is small, then adjusting the weights in the current layer will be small. it causes the gradient to shrink exponentially when it goes to the input layer. so that when in the input layer it fails to do the learning due to <em>vanishing gradient problems</em>. so the model <strong>fails to learn</strong> when a pass forward is made again to make predictions.</p>
<p><strong>Exploding Gradient Descent</strong>: the update value obtained will exponentially increase when heading to the input layer. The characteristics of the model have an exploding gradient problem, which is when the cost function results are NaN.</p>
<p><img src="img/lstm_text/lossnan.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Here is a link duscussion regarding to exploding grgadient descent: <a href="https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network">nan loss when training NN</a></p>
</div>
</div>
<div id="recurrent-neural-network" class="section level1">
<h1>Recurrent Neural Network</h1>
<p>From the vasishing / exploding gradient problem mentioned above, the development of architecture from the RNN, namely LSTM and GRU, is able to handle the problem. <em>will be discussed below</em>. RNN itself has not been able to handle vanishing gradients due to short-term memory problems.</p>
<p>So what is RNN? RNN is a deep learning architecture model that is commonly used for sequential data. What’s the sequential data? The following are examples of sequential data cases:</p>
<ol style="list-style-type: decimal">
<li><strong>Sentiment classification</strong>. Input: text, output: rating/sentiment class.</li>
<li><strong>Translator</strong>. Input: text, output: text translator.</li>
<li><strong>Time series data</strong>: input: Numeric, output: forecasting result.</li>
<li><strong>Speech Recognation</strong>: input: audio, output: text.</li>
</ol>
<div id="rnn-concept" class="section level2">
<h2>RNN Concept</h2>
<p>The main idea of RNN is to utilize sequential information processing. RNN is called repetitive because it performs the same task for each successive element, with output depending on the previous calculation.</p>
<p><img src="img/lstm_text/rnn.jpg" width="60%" style="display: block; margin: auto;" /></p>
<p>The diagram above is the architecture of RNN after opening it unfolded. Suppose we have a sentence consisting of 10 words, it means that there will be 10 NN-layers formed separately. Each layer represents each word. The following are some explanations of the notation in the diagram above:</p>
<ul>
<li><span class="math inline">\(x_t\)</span> is an input at (time) <span class="math inline">\(t\)</span>.</li>
<li>RNN stores an internal state <span class="math inline">\(S_t\)</span> which becoming a <em>memory</em> in RNN. <span class="math inline">\(S_t\)</span> is calculated based on the previous hidden state and the input in the current step. The activation function of <span class="math inline">\(f\)</span> is usually nonlinear like <strong>tanh</strong> or <strong>ReLU</strong>.</li>
</ul>
<center>
<span class="math inline">\(S_t = f(U_{xt} + W_{St} − 1)\)</span>.
</center>
<p><br></p>
<ul>
<li><span class="math inline">\(O_t\)</span> is an output from each step <span class="math inline">\(t\)</span></li>
</ul>
<div id="forward-pass" class="section level3">
<h3>Forward Pass</h3>
<p>For example case studies related to sentiment classification. Then the input will contain one-hot vectors of words, and the output is a sentiment class. So a feed-forward pass scheme is performed as:</p>
<p><img src="img/lstm_text/forwardpass.PNG" width="60%" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li>Each layer describes each word.</li>
<li>For an explanation of the concept of the Forward pass, we look at the layer when <span class="math inline">\(t\)</span> (in the middle).</li>
</ol>
<ul>
<li>Remember that the RNN pays attention to the output calculation data at <span class="math inline">\(t - 1\)</span>. So initially we have to calculate the state at <span class="math inline">\(S_t\)</span> first. Involving the multiplication of the input matrix <span class="math inline">\(x_t\)</span> with the parameter <span class="math inline">\(U\)</span> and summing the results of the product <span class="math inline">\(s_{t1}\)</span>. Then the results are processed with the tanh activation function. The calculation details are as follows:</li>
</ul>
<center>
<span class="math inline">\(s_t=tanh(U.x_t+W.s_{t−1)}\)</span>
</center>
<p><br></p>
<ul>
<li>The results from <span class="math inline">\(s_t\)</span> are then passed to the output by tiling the matrix multiplication of the <span class="math inline">\(V\)</span> parameters and then passed with the softmax activation function. The details are as follows:</li>
</ul>
<center>
<span class="math inline">\(\hat{y}_t=softmax(V.s_t)\)</span>
</center>
<p><br></p>
<p>The above process is often illustrated like:</p>
<p><img src="img/lstm_text/rnn-network.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="backpropogation-through-time" class="section level3">
<h3>Backpropogation Through Time</h3>
<p>The purpose of the RNN model training is to find the parameters <span class="math inline">\(U, V,\)</span> and <span class="math inline">\(W\)</span> which produce minimum errors. The term BTT arises because the RNN architecture pays attention to the previous time series. So to calculate the gradient at the time step <span class="math inline">\(t\)</span>, we must calculate the speech at step <span class="math inline">\(t − 1, t − 2, t − 3\)</span> until it is at time <span class="math inline">\(t = 1\)</span>. If you are curious about how this algorithm works, I suggest to read more on this article: <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">Part 3: Backpropogarion &amp; Vanishing Gradient</a>.</p>
<p><img src="img/lstm_text/btt.gif" width="35%" style="display: block; margin: auto;" /></p>
<p>As mentioned in the previous chapter, the RNN model also has a vanishing gradient problem, because it cannot capture long-term dependencies. because the number of layers is too long, making the backprop process produces a gradient value that is getting smaller and even close to zero or is said to disappear when it arrives at the initial layers.</p>
<blockquote>
<p>This is caused by the multiplication properties between fractions. Imagine that for example a fraction is 1/4, multiplied by another fraction such as 1/3, then in one operation the value is 1/12. Multiplied by other fractions such as 1/5, the value becomes 1/60, etc. This value will shrink exponentially, and with a small fractional value and many multiplication operations, the value will be close to zero.</p>
</blockquote>
<p>To overcome this problem, there is a development of RNN model namely <strong>Long-Term Short Memory (LSTM)</strong>.</p>
</div>
</div>
</div>
<div id="lstm" class="section level1">
<h1>LSTM</h1>
<p>Just like RNN, LSTM has a sequential model which is illustrated with a green box. if unfolded the architecture becomes as below:</p>
<p><img src="img/lstm_text/lstm.jpg" width="70%" style="display: block; margin: auto;" /></p>
<p>The difference between RNN and LSTM is that it has additional signal information that is given from one time step to the next time step which is commonly called “<strong>cell memory</strong>”. LSTM is designed to overcome the problem of vanishing gradient, using the <strong>gate mechanism</strong>.</p>
<div id="lstm-network" class="section level2">
<h2>LSTM Network</h2>
<p>So the components in LSTM consist of:</p>
<ul>
<li><strong>Forget Gate</strong> f (NN with <em>sigmoid</em> as activation function).</li>
<li><strong>Candidate Layer</strong> g (NN with <em>tanh</em> as activation function).</li>
<li><strong>Input Gate</strong> I (NN with <em>sigmoid</em> as activation function).</li>
<li><strong>Output Gate</strong> O (NN with <em>sigmoid</em> as activation function).</li>
<li><strong>Hidden State</strong> H (vector).</li>
<li><strong>Memory State</strong> C (vector).</li>
</ul>
<p>The following is the LSTM diagram at the t-time step.</p>
<p><img src="img/lstm_text/lstmnet.png" width="70%" style="display: block; margin: auto;" /></p>
<ul>
<li><span class="math inline">\(X_t\)</span> = Input vector at the t-time.</li>
<li><span class="math inline">\(H_{t−1}\)</span> = Previous Hidden state.</li>
<li><span class="math inline">\(C_{t−1}\)</span> = Previous Memory state.</li>
<li><span class="math inline">\(H_t\)</span> = Current Hidden state.</li>
<li><span class="math inline">\(C_t\)</span> = Current Memori state.</li>
<li>[*] = multiplication operation.</li>
<li>[+] = addition operation.</li>
</ul>
<p>so the input of each LSTM module is <span class="math inline">\(X_t\)</span> (current input), <span class="math inline">\(H_{t − 1}\)</span>, and <span class="math inline">\(C_{t − 1}\)</span>. then the output is <span class="math inline">\(H_t\)</span>, and <span class="math inline">\(C_t\)</span>.</p>
<p>extracted from <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">Denny Britz’s article</a> which is a summary of Christopher Olah’s article: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(l, f, O\)</span> is the Input, Forget, and Output gates. Both input, forget, and output have the same function formula (sigmoid), which only distinguishes the matrix parameters (<strong>note the formula below</strong>). This means that the output of this gate has a vector of values between 0 to 1. zero means that the information is blocked completely, and one means that all information is included. The gate input controls how many states you have just computed for the current input that you want to let pass. The forget gate controls how many previous states you want to let pass. Finally, the gate output controls how many internal states you want to expose to the network (higher layer &amp; next time step). All gates have the same dimensions as the hidden state dimension (etc.) as a measure for the hidden state. The output of the sigmoid gate will be multiplied by another value to control how much that value is used.</li>
</ol>
<center>
<span class="math inline">\(I= \sigma(x_tU^I + s_{t-_1}W^I)\)</span> <br>
<span class="math inline">\(f= \sigma(x_tU^f + s_{t-_1}W^f)\)</span> <br>
<span class="math inline">\(O= \sigma(x_tU^O + s_{t-_1}W^O)\)</span> <br>
</center>
<blockquote>
<p>value 1 means “<em>really take care of this element</em>” while 0 means “<em>completely get rid of this element</em>”.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li><p><span class="math inline">\(g\)</span> is a “candidate” hidden state that is computed based on the current input and the previous hidden state.</p></li>
<li><p><span class="math inline">\(c_t\)</span> is the internal memory of the unit. It is a combination of the previous memory <span class="math inline">\(c_{t-1}\)</span> multiplied by the forget gate, and the newly computed hidden state g, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input.</p></li>
</ol>
<center>
<span class="math inline">\(c_t=f_t*c_{t-_1} + I_t*g\)</span>
</center>
</div>
</div>
<div id="implementation-lstm-with-keras" class="section level1">
<h1>Implementation LSTM with Keras</h1>
<pre class="r"><code># load packages required
library(keras)
library(RVerbalExpressions)
library(magrittr)
library(textclean)
library(tidyverse)
library(tidytext)
library(rsample)
library(yardstick)
library(caret)

#set seed keras for reproducible result
use_session_with_seed(2)

# set conda env
use_condaenv(&quot;tensorflow&quot;)</code></pre>
<div id="import-data" class="section level2">
<h2>Import Data</h2>
<p>In this example we will use a case study of sentiment tweets about airlines in the US obtained from Kaggle.</p>
<pre class="r"><code>data &lt;- read_csv(&quot;data_input/tweets.csv&quot;)
glimpse(data)</code></pre>
<pre><code>## Observations: 14,640
## Variables: 15
## $ tweet_id                     &lt;dbl&gt; 5.703061e+17, 5.703011e+17, 5.703...
## $ airline_sentiment            &lt;chr&gt; &quot;neutral&quot;, &quot;positive&quot;, &quot;neutral&quot;,...
## $ airline_sentiment_confidence &lt;dbl&gt; 1.0000, 0.3486, 0.6837, 1.0000, 1...
## $ negativereason               &lt;chr&gt; NA, NA, NA, &quot;Bad Flight&quot;, &quot;Can&#39;t ...
## $ negativereason_confidence    &lt;dbl&gt; NA, 0.0000, NA, 0.7033, 1.0000, 0...
## $ airline                      &lt;chr&gt; &quot;Virgin America&quot;, &quot;Virgin America...
## $ airline_sentiment_gold       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N...
## $ name                         &lt;chr&gt; &quot;cairdin&quot;, &quot;jnardino&quot;, &quot;yvonnalyn...
## $ negativereason_gold          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, N...
## $ retweet_count                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ text                         &lt;chr&gt; &quot;@VirginAmerica What @dhepburn sa...
## $ tweet_coord                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, N...
## $ tweet_created                &lt;chr&gt; &quot;2015-02-24 11:35:52 -0800&quot;, &quot;201...
## $ tweet_location               &lt;chr&gt; NA, NA, &quot;Lets Play&quot;, NA, NA, NA, ...
## $ user_timezone                &lt;chr&gt; &quot;Eastern Time (US &amp; Canada)&quot;, &quot;Pa...</code></pre>
</div>
<div id="text-pre-processing" class="section level2">
<h2>Text Pre-Processing</h2>
<div id="setup-regex" class="section level3 tabset">
<h3>Setup regex</h3>
<div id="mention" class="section level4">
<h4>mention</h4>
<pre class="r"><code>mention &lt;- rx() %&gt;% 
  rx_find(value = &quot;@&quot;) %&gt;% 
  rx_alnum() %&gt;% 
  rx_one_or_more()
mention</code></pre>
<pre><code>## [1] &quot;(@)[A-z0-9]+&quot;</code></pre>
<pre class="r"><code>&quot;@VirginAmerica What @dhepburn said.&quot; %&gt;% 
  str_remove_all(pattern = mention) %&gt;% 
  str_squish()</code></pre>
<pre><code>## [1] &quot;What said.&quot;</code></pre>
</div>
<div id="hashtag" class="section level4">
<h4>hashtag</h4>
<pre class="r"><code>hashtag &lt;- rx() %&gt;% 
  rx_find(value = &quot;#&quot;) %&gt;% 
  rx_alnum() %&gt;% 
  rx_one_or_more()
hashtag</code></pre>
<pre><code>## [1] &quot;(#)[A-z0-9]+&quot;</code></pre>
<pre class="r"><code>&quot;@VirginAmerica I&#39;m #elevategold for a good reason: you rock!!&quot; %&gt;% 
  str_remove_all(pattern = mention) %&gt;%
  str_remove_all(pattern = hashtag) %&gt;% 
  str_squish()</code></pre>
<pre><code>## [1] &quot;I&#39;m for a good reason: you rock!!&quot;</code></pre>
</div>
<div id="question-mark" class="section level4">
<h4>question mark</h4>
<pre class="r"><code>question &lt;- rx() %&gt;% 
  rx_find(value = &quot;?&quot;) %&gt;% 
  rx_one_or_more()
question</code></pre>
<pre><code>## [1] &quot;(\\?)+&quot;</code></pre>
</div>
<div id="exclamation-mark" class="section level4">
<h4>exclamation mark</h4>
<pre class="r"><code>exclamation &lt;- rx() %&gt;% 
  rx_find(value = &quot;!&quot;) %&gt;% 
  rx_one_or_more()
exclamation</code></pre>
<pre><code>## [1] &quot;(!)+&quot;</code></pre>
</div>
<div id="punctuation" class="section level4">
<h4>punctuation</h4>
<pre class="r"><code>punctuation &lt;- rx_punctuation()
punctuation</code></pre>
<pre><code>## [1] &quot;[[:punct:]]&quot;</code></pre>
</div>
<div id="number" class="section level4">
<h4>number</h4>
<pre class="r"><code>number &lt;- rx_digit()
number</code></pre>
<pre><code>## [1] &quot;\\d&quot;</code></pre>
</div>
<div id="dollar-sign" class="section level4">
<h4>dollar sign</h4>
<pre class="r"><code>dollar &lt;- rx() %&gt;% 
  rx_find(&quot;$&quot;)
dollar</code></pre>
<pre><code>## [1] &quot;(\\$)&quot;</code></pre>
</div>
</div>
<div id="text-cleansing" class="section level3 tabset">
<h3>Text Cleansing</h3>
<div id="replace_url" class="section level4">
<h4><code>replace_url</code></h4>
<pre class="r"><code>&quot;@VirginAmerica Really missed a prime opportunity, there. https://t.co/mWpG7grEZP&quot; %&gt;% 
  replace_url()</code></pre>
<pre><code>## [1] &quot;@VirginAmerica Really missed a prime opportunity, there. &quot;</code></pre>
</div>
<div id="replace_emoticon" class="section level4">
<h4><code>replace_emoticon</code></h4>
<pre class="r"><code>&quot;@SouthwestAir thanks! Very excited to see it :3&quot; %&gt;%
  replace_emoticon()</code></pre>
<pre><code>## [1] &quot;@SouthwestAir thanks! Very excited to see it smiley &quot;</code></pre>
</div>
<div id="replace_contruction" class="section level4">
<h4><code>replace_contruction</code></h4>
<pre class="r"><code>&quot;@united I&#39;d thank you - but you didn&#39;t help. taking 6 hours to reply isn&#39;t actually helpful&quot; %&gt;% 
  replace_contraction()</code></pre>
<pre><code>## [1] &quot;@united I would thank you - but you did not help. taking 6 hours to reply is not actually helpful&quot;</code></pre>
</div>
<div id="replace_word_elongation" class="section level4">
<h4><code>replace_word_elongation</code></h4>
<pre class="r"><code>&quot;@VirginAmerica heyyyy guyyyys.. :/&quot; %&gt;% 
  replace_word_elongation()</code></pre>
<pre><code>## [1] &quot;@VirginAmerica hey guys.. :/&quot;</code></pre>
<pre class="r"><code>data &lt;- data %&gt;% 
  mutate(
    text_clean = text %&gt;% 
      replace_url() %&gt;% 
      replace_emoji() %&gt;% 
      replace_emoticon() %&gt;% 
      replace_html() %&gt;% 
      str_remove_all(pattern = mention) %&gt;% 
      str_remove_all(pattern = hashtag) %&gt;% 
      replace_contraction() %&gt;% 
      replace_word_elongation() %&gt;% 
      str_replace_all(pattern = question, replacement = &quot;questionmark&quot;) %&gt;% 
      str_replace_all(pattern = exclamation, replacement = &quot;exclamationmark&quot;) %&gt;% 
      str_remove_all(pattern = punctuation) %&gt;% 
      str_remove_all(pattern = number) %&gt;% 
      str_remove_all(pattern = dollar) %&gt;% 
      str_to_lower() %&gt;% 
      str_squish()
  )</code></pre>
<pre class="r"><code>data %&gt;% 
  select(text, text_clean) %&gt;% 
  sample_n(20)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    text                                text_clean                          
##    &lt;chr&gt;                               &lt;chr&gt;                               
##  1 @united How can I file a claim whe~ how can i file a claim when your ag~
##  2 @USAirways Seriously. You can&#39;t tw~ seriously you can not tweet and let~
##  3 @JetBlue thank you will do!! You g~ thank you will doexclamationmark yo~
##  4 @united what&#39;s the hold up with fl~ what is the hold up with flight fro~
##  5 @AmericanAir ticks me off.          ticks me off                        
##  6 @AmericanAir but at least your aut~ but at least your automated message~
##  7 @united the hotel you sent us to w~ the hotel you sent us to wouldnt ta~
##  8 @AmericanAir your definition and m~ your definition and mine of min is ~
##  9 @JetBlue why are you always so ama~ why are you always so amazingexclam~
## 10 @JetBlue and The from @WSJ Team to~ and the from team to offer in acces~
## 11 @JetBlue Oh that totally looks on ~ oh that totally looks on par with s~
## 12 &quot;@united Thank you!! \U0001f60a&quot;    thank youexclamationmark smiling fa~
## 13 @USAirways Still irritated that ou~ still irritated that our well plann~
## 14 @united Sure.  Follow for a sec an~ sure follow for a sec and i will    
## 15 &quot;@SouthwestAir your A-list program~ your alist program is a complete jo~
## 16 @AmericanAir Thanks, both airlines~ thanks both airlines said that it i~
## 17 @AmericanAir Thank you. Good sugge~ thank you good suggestion i checked~
## 18 @united care less about the person~ care less about the person although~
## 19 @SouthwestAir I was helped by a ni~ i was helped by a nice lady at phx ~
## 20 @united we&#39;ve been here since 3am ~ we have been here since am and you ~</code></pre>
</div>
</div>
<div id="prepare-datainput" class="section level3">
<h3>prepare datainput</h3>
<pre class="r"><code>data &lt;- data %&gt;% 
  mutate(label = factor(airline_sentiment, levels = c(&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;)),
         label = as.numeric(label),
         label = label - 1) %&gt;% 
  select(text_clean, label) %&gt;% 
  na.omit()
head(data, 10)</code></pre>
<pre><code>## # A tibble: 10 x 2
##    text_clean                                                         label
##    &lt;chr&gt;                                                              &lt;dbl&gt;
##  1 what said                                                              1
##  2 plus you have added commercials to the e tongue sticking out erie~     2
##  3 i did not today must mean i need to take another tripexclamationm~     1
##  4 it is really aggressive to blast obnoxious entertainment in your ~     0
##  5 and it is a really big bad thing about it                              0
##  6 seriously would pay a flight for seats that did not have this pla~     0
##  7 yes nearly every time i fly vx this ear worm won t go away smiley      2
##  8 really missed a prime opportunity for men without hats parody the~     1
##  9 well i did not but now i doexclamationmark laughing                    2
## 10 it was amazing and arrived an hour early you are too good to me        2</code></pre>
</div>
</div>
<div id="tokenizer" class="section level2">
<h2>Tokenizer</h2>
<p>Tokenizer aims to separate each word in the entire document into a token form. The <code>num_words</code> parameter is for setting the maximum number of words to be used, sorted according to the largest frequency order. words that rarely appear will be removed. from a total of 13291 unique words contained in the text data, we reduced it to 1024 which will be used to make the model. The lower parameter is a logic condition, if TRUE then all words will be transformed to lowercase (tolower).</p>
<pre class="r"><code>num_words &lt;- 1024 

# prepare tokenizers
tokenizer &lt;- text_tokenizer(num_words = num_words,
                            lower = TRUE) %&gt;% 
  fit_text_tokenizer(data$text_clean)</code></pre>
<pre class="r"><code>paste(&quot;number of unique words:&quot;, length(tokenizer$word_counts))</code></pre>
<pre><code>## [1] &quot;number of unique words: 13291&quot;</code></pre>
<div id="intuition" class="section level3">
<h3>Intuition</h3>
<p>Suppose we have 5 pieces of text documents that are stored in the docs object. Then we made a token with a maximum of words / terms used, which is 4. It means that words that rarely appear will not be used during the train model. To see the number of unique words stored in the document dictionary, use the command <code>token$word_counts</code>. To see the list of words with the highest frequency of appearances, use the <code>token$word_index</code> command.</p>
<pre class="r"><code>docs &lt;- c(&#39;Well done!&#39;,
        &#39;Good work&#39;,
        &#39;Great effort&#39;,
        &#39;nice work&#39;,
        &#39;Excellent!&#39;)
tokendocs &lt;- text_tokenizer(num_words = 4, 
                            lower = TRUE) %&gt;% 
  fit_text_tokenizer(docs)</code></pre>
<pre class="r"><code>paste(&quot;number of unique words&quot;,length(tokendocs$word_counts))</code></pre>
<pre><code>## [1] &quot;number of unique words 8&quot;</code></pre>
<pre class="r"><code>tokendocs$word_index[1:4]</code></pre>
<pre><code>## $work
## [1] 1
## 
## $well
## [1] 2
## 
## $done
## [1] 3
## 
## $good
## [1] 4</code></pre>
</div>
</div>
<div id="splitting-data" class="section level2">
<h2>Splitting Data</h2>
<p>Splitting data will be done into 3 parts, namely train, validation, and test. The proportion is 60% for trains and the remaining 40% is in partitions for data validation and testing.</p>
<p><img src="img/lstm_text/split.PNG" width="70%" style="display: block; margin: auto;" /></p>
<p>Data Train is the data that we will use to train the model. Data Validation for evaluating hyperparameter tuning in models (adjust hidden layers, optimizers, learning rates, etc.). While the test data as an evaluator of the model that we make on <em>unseen data</em>.</p>
<pre class="r"><code>set.seed(100)
intrain &lt;- initial_split(data = data, prop = 0.8, strata = &quot;label&quot;)

data_train &lt;- training(intrain)
data_test &lt;- testing(intrain)

set.seed(100)
inval &lt;- initial_split(data = data_test, prop = 0.5, strata = &quot;label&quot;)

data_val &lt;- training(inval)
data_test &lt;- testing(inval)</code></pre>
<pre class="r"><code>maxlen &lt;- max(str_count(data$text_clean, &quot;\\w+&quot;)) + 1 
paste(&quot;maxiumum length words in data:&quot;, maxlen)</code></pre>
<pre><code>## [1] &quot;maxiumum length words in data: 90&quot;</code></pre>
<pre class="r"><code># prepare x
data_train_x &lt;- texts_to_sequences(tokenizer, data_train$text_clean) %&gt;%
  pad_sequences(maxlen = maxlen)

data_val_x &lt;- texts_to_sequences(tokenizer, data_val$text_clean) %&gt;%
  pad_sequences(maxlen = maxlen)

data_test_x &lt;- texts_to_sequences(tokenizer, data_test$text_clean) %&gt;%
  pad_sequences(maxlen = maxlen)

# prepare y
data_train_y &lt;- to_categorical(data_train$label, num_classes = 3)
data_val_y &lt;- to_categorical(data_val$label, num_classes = 3)
data_test_y &lt;- to_categorical(data_test$label, num_classes = 3)</code></pre>
<div id="intuition-1" class="section level3">
<h3>Intuition</h3>
<p>Command <code>texts_to_sequence()</code> aims to create a matrix results of the transformation text to the form of a number sequence (integer). Then wrapped with the command <code>pad_sequences()</code> which aims to equalize the dimensions of the length on the entire document. Imagine the input layer of a matrix, it must have the same row and column. Therefore it is necessary to do <em>padding</em>. By default the value parameter will be set to 0. This means that if there are words that are not in our token (which has been limited by <code>num_words</code>) then it will be transformed to 0. The following is the illustration:</p>
<pre class="r"><code>texts_to_sequences(tokendocs, c(&quot;Excellent!&quot;, 
                                &quot;Good job bro, keep hard work&quot;, 
                                &quot;well done&quot;)) %&gt;% 
  pad_sequences(maxlen = 5)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    0    0    0    0    1
## [3,]    0    0    0    2    3</code></pre>
<p>The result of <code>text_to_sequences</code> is a matrix of size <span class="math inline">\(n ∗ maxlen\)</span>. The example above consists of 3 text documents and is set to maxlen = 5. it will produce a <span class="math inline">\(3 \times 5\)</span> matrix which each index is a representative integer of the same words as <code>tokendocs</code> in the i-th list. Recall, the word <code>done</code> is on the 3rd list on our token, right? therefore the matrix result above in the third document and the last integer sequence is 3. Why does it appear in the last index? because in <code>pad_sequences</code> we don’t set parameters of the padding type whether “pre” or “post” and by default is “pre”.</p>
<pre class="r"><code>tokendocs$word_index[3]</code></pre>
<pre><code>## $done
## [1] 3</code></pre>
</div>
</div>
<div id="architecture" class="section level2">
<h2>Architecture</h2>
<div id="embedding-layer" class="section level3">
<h3>Embedding Layer</h3>
<p>Embedding Layers can only be used in the initial / first layer of the LSTM architecture. In a variety of deep learning frameworks such as Keras, the embedding layer aims to train text data into numerical vectors which represent the closeness of the meaning of each word.</p>
<p>Embedding layer accepts several parameters. Some examples are:
* <code>input_dim</code>, which is the maximum dimension of the vocabulary that has been explained in the <strong>num_words</strong> section.</p>
<ul>
<li><p><code>input_length</code>, the maximum length of the word sequence in the document input.</p></li>
<li><p><code>output_dim</code> which is the embedding dimension of the output layer which will be passed to the next layer. generally is 32, but can be more dependent on the problem we face.</p></li>
</ul>
<blockquote>
<p>Input received of 2D vectors with the form: {batch_size, sequence_length}, while the output received 3D tensor with the forms {batch_size, sequence_length, output_dim}.</p>
</blockquote>
</div>
<div id="deep-neural-layer" class="section level3">
<h3>Deep Neural Layer</h3>
<p>The Deep Network Layer accepts the embedding matrix as input and then is converted into smaller dimensions. The dimensions of the compression results have represented information from the data. In the case of data text, the deep learning architecture commonly used is RNN &gt; LSTM / GRU.</p>
<blockquote>
<p>you can check the <a href="https://keras.io/preprocessing/sequence/">Keras Documentation</a> for the details sequential layers.</p>
</blockquote>
</div>
<div id="output-layer" class="section level3">
<h3>Output Layer</h3>
<p>This output layer is the last layer in the deep learning architecture. At Keras use the layer_dense command where we need to set the unit parameters or how many neurons we want to build. In this case I use 3 units, because there are 3 classes we have (negative, neutral, positive).</p>
</div>
<div id="random-initialization" class="section level3">
<h3>Random Initialization</h3>
<p>When the neural network / deep learning model train often results in different results. Why? because NN and DL use weigth which is generated randomly (randomness initialization). therefore we need to set the numbers (x-random models) in order to get a fixed result when repeated in the train (reproducible result). this can be done with the seed parameter in the <code>initializer_random_uniform</code> command. for more details, read the question and answer article in <a href="https://keras.rstudio.com/articles/faq.html">Keras studio</a></p>
<pre class="r"><code># initiate keras model sequence
model &lt;- keras_model_sequential()

# model
model %&gt;%
  # layer input
  layer_embedding(
    name = &quot;input&quot;,
    input_dim = num_words,
    input_length = maxlen,
    output_dim = 32, 
    embeddings_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %&gt;%
  # layer dropout
  layer_dropout(
    name = &quot;embedding_dropout&quot;,
    rate = 0.5
  ) %&gt;%
  # layer lstm 1
  layer_lstm(
    name = &quot;lstm&quot;,
    units = 256,
    dropout = 0.2,
    recurrent_dropout = 0.2,
    return_sequences = FALSE, 
    recurrent_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2),
    kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %&gt;%
  # layer output
  layer_dense(
    name = &quot;output&quot;,
    units = 3,
    activation = &quot;softmax&quot;, 
    kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  )</code></pre>
<ul>
<li>Dropout parameters are added to reduce the risk of overfit. the range of dropout values between 0 to 1. commonly used is 0.2 to 0.5. the closer to 0 will tend to overfit, while the closer to 1 has the risk of underfit.</li>
</ul>
</div>
<div id="compiling-model" class="section level3">
<h3>Compiling Model</h3>
<p>For two category classes, the lost function used is <code>binary_crossentropy</code> while for multiclass cases it uses <code>categorical_crossentropy</code>. There are not only 2 option, but the most common when working with classification cases, these 2 loss functions are used. Here are some loss function options from <a href="https://keras.io/losses/">Keras Documentation</a></p>
<pre class="r"><code># compile the model
model %&gt;% compile(
  optimizer = &quot;adam&quot;,
  metrics = &quot;accuracy&quot;,
  loss = &quot;categorical_crossentropy&quot;
)

# model summary
summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## input (Embedding)                (None, 90, 32)                32768       
## ___________________________________________________________________________
## embedding_dropout (Dropout)      (None, 90, 32)                0           
## ___________________________________________________________________________
## lstm (LSTM)                      (None, 256)                   295936      
## ___________________________________________________________________________
## output (Dense)                   (None, 3)                     771         
## ===========================================================================
## Total params: 329,475
## Trainable params: 329,475
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
</div>
</div>
<div id="train-the-model" class="section level2">
<h2>Train the Model</h2>
<pre class="r"><code># model fit settings
epochs &lt;- 10
batch_size &lt;- 512

# fit the model
history &lt;- model %&gt;% fit(
  data_train_x, data_train_y,
  batch_size = batch_size, 
  epochs = epochs,
  verbose = 1,
  validation_data = list(
    data_val_x, data_val_y
  )
)

# history plot
plot(history)</code></pre>
<p><img src="/posts/2019-09-21-text-lstm_files/figure-html/unnamed-chunk-41-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<pre class="r"><code># predict on train
data_train_pred &lt;- model %&gt;%
  predict_classes(data_train_x) %&gt;%
  as.vector()

# predict on val
data_val_pred &lt;- model %&gt;%
  predict_classes(data_val_x) %&gt;%
  as.vector()

# predict on test
data_test_pred &lt;- model %&gt;%
  predict_classes(data_test_x) %&gt;%
  as.vector()</code></pre>
<pre class="r"><code># accuracy on data train
accuracy_vec(
 truth = factor(data_train$label,labels = c(&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;)),
 estimate = factor(data_train_pred, labels = c(&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;))
)</code></pre>
<pre><code>## [1] 0.8056534</code></pre>
<pre class="r"><code># accuracy on data test
accuracy_vec(
 truth = factor(data_test$label,labels = c(&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;)),
 estimate = factor(data_test_pred, labels = c(&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;))
)</code></pre>
<pre><code>## [1] 0.7895461</code></pre>
</div>
</div>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://ahmadhusain.in/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://ahmadhusain.in/tags/classification/">Classification</a></li>
      <li><a href="http://ahmadhusain.in/tags/supervised-learning/">Supervised Learning</a></li>
      <li><a href="http://ahmadhusain.in/tags/nlp/">NLP</a></li>
      <li><a href="http://ahmadhusain.in/tags/lstm/">LSTM</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://ahmadhusain.in/post/2018-09-15-reproduce-economist-plot/">
    <span class="title">Next Page »</span>
    <br>
    <span>Reproduce Economist Plot</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on twitter"
        href="https://twitter.com/intent/tweet/?text=Text%20Classification%20with%20LSTM&amp;url=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f&amp;hashtags=DeepLearning%2cClassification%2cSupervisedLearning%2cNLP%2cLSTM">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f&amp;title=Text%20Classification%20with%20LSTM&amp;summary=Text%20Classification%20with%20LSTM&amp;source=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on reddit"
        href="https://reddit.com/submit?url=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f&title=Text%20Classification%20with%20LSTM">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on facebook"
        href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on whatsapp"
        href="https://api.whatsapp.com/send?text=Text%20Classification%20with%20LSTM%20-%20http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Text Classification with LSTM on telegram"
        href="https://telegram.me/share/url?text=Text%20Classification%20with%20LSTM&amp;url=http%3a%2f%2fahmadhusain.in%2fpost%2ftext-lstm%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="http://ahmadhusain.in/">Husain&#39;s Blog</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        container.appendChild(copybutton);
    });
</script>
</body>

</html>
